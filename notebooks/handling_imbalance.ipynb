{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74df29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting balancing techniques application in parallel...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.ensemble import EasyEnsembleClassifier, RUSBoostClassifier\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "import concurrent.futures\n",
    "\n",
    "# Base directory\n",
    "base_dir = r\"C:\\Machine Learning\\Phase 2\"\n",
    "\n",
    "# Define paths to deduplicated datasets\n",
    "dedup_approaches = [\"Direct_Removal\", \"Instance_Weighting\", \"Train_Test_Aware\"]\n",
    "outlier_techniques = [\"Direct_Removal\", \"Isolation_Forest\", \"Log1p_Winsorization\", \"Winsorization\", \"Z-Score_Trimming\"]\n",
    "scaling_techniques = [\"MinMaxScaler\", \"PowerTransformer\", \"QuantileTransformer\", \"RobustScaler\", \"StandardScaler\"]\n",
    "balancing_techniques = [\"SMOTETomek\", \"ClassWeights\", \"EasyEnsemble\", \"RUSBoost\", \"ThresholdTuning\"]\n",
    "\n",
    "# Create a cache directory for checkpoints\n",
    "cache_dir = os.path.join(base_dir, \"cache\")\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "class ThresholdTuningClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"A simple wrapper class for threshold tuning\"\"\"\n",
    "    def __init__(self, base_estimator=None, threshold=0.5):\n",
    "        self.threshold = threshold\n",
    "        self.base_estimator = base_estimator or RandomForestClassifier(random_state=42)\n",
    "        self.classes_ = np.array([0, 1])\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        self.base_estimator.fit(X, y)\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "        probs = self.base_estimator.predict_proba(X)[:, 1]\n",
    "        return np.where(probs > self.threshold, 1, 0)\n",
    "\n",
    "def apply_balancing_technique(args):\n",
    "    \"\"\"Apply a single balancing technique to a specific dataset\"\"\"\n",
    "    dedup_approach, balancing_technique, x_train, y_train, target_dir, x_train_filename = args\n",
    "    \n",
    "    start_time = time.time()\n",
    "    balanced_y_file = f\"phase2_{dedup_approach}_y_train_{balancing_technique}.csv\"\n",
    "    \n",
    "    # Check if work is already done for this specific combination\n",
    "    checkpoint_file = os.path.join(cache_dir, f\"checkpoint_{dedup_approach}_{balancing_technique}_{os.path.basename(target_dir)}.txt\")\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        print(f\"Skipping {balancing_technique} for {target_dir}, already processed\")\n",
    "        return\n",
    "    \n",
    "    # Apply specific balancing technique\n",
    "    try:\n",
    "        if balancing_technique == \"SMOTETomek\":\n",
    "            smote_tomek = SMOTETomek(random_state=42, n_jobs=-1)  # Use all available cores\n",
    "            _, y_resampled = smote_tomek.fit_resample(x_train, y_train.iloc[:, 0])\n",
    "            balanced_y = pd.DataFrame(y_resampled)\n",
    "        \n",
    "        elif balancing_technique == \"ClassWeights\":\n",
    "            # For class weights, we don't actually change y_train\n",
    "            # but create a weights column that can be used during training\n",
    "            class_counts = y_train.iloc[:, 0].value_counts()\n",
    "            n_samples = len(y_train)\n",
    "            weights = {cls: n_samples / (len(class_counts) * count) for cls, count in class_counts.items()}\n",
    "            y_weights = y_train.iloc[:, 0].map(weights)\n",
    "            balanced_y = pd.concat([y_train, pd.DataFrame({'weight': y_weights})], axis=1)\n",
    "        \n",
    "        elif balancing_technique == \"EasyEnsemble\":\n",
    "            ensemble = EasyEnsembleClassifier(random_state=42, n_estimators=10)\n",
    "            ensemble.fit(x_train, y_train.iloc[:, 0])\n",
    "            # For demonstration, just use the original y_train\n",
    "            balanced_y = y_train\n",
    "        \n",
    "        elif balancing_technique == \"RUSBoost\":\n",
    "            rusboost = RUSBoostClassifier(random_state=42, n_estimators=50)\n",
    "            rusboost.fit(x_train, y_train.iloc[:, 0])\n",
    "            # For demonstration, just use the original y_train\n",
    "            balanced_y = y_train\n",
    "        \n",
    "        elif balancing_technique == \"ThresholdTuning\":\n",
    "            # For threshold tuning, we don't actually change y_train\n",
    "            threshold_tuner = ThresholdTuningClassifier(threshold=0.3)\n",
    "            threshold_tuner.fit(x_train, y_train.iloc[:, 0])\n",
    "            balanced_y = y_train\n",
    "        \n",
    "        # Save directly to the target directory\n",
    "        output_path = os.path.join(target_dir, balanced_y_file)\n",
    "        balanced_y.to_csv(output_path, index=False)\n",
    "        \n",
    "        # Create checkpoint file to mark as complete\n",
    "        with open(checkpoint_file, 'w') as f:\n",
    "            f.write(f\"completed at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"{balancing_technique} for {os.path.basename(target_dir)} took {elapsed:.2f} seconds\")\n",
    "        \n",
    "        return output_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {balancing_technique} for {target_dir}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_dedup_approach(dedup_approach):\n",
    "    \"\"\"Process all balancing techniques for a single deduplication approach\"\"\"\n",
    "    try:\n",
    "        # First, load the original y_train\n",
    "        y_train_path = os.path.join(\n",
    "            base_dir, \n",
    "            \"Data\", \n",
    "            \"deduplicated_datasets\", \n",
    "            dedup_approach, \n",
    "            f\"phase2_{dedup_approach}_y_train.csv\"\n",
    "        )\n",
    "        \n",
    "        if not os.path.exists(y_train_path):\n",
    "            print(f\"Warning: {y_train_path} does not exist. Skipping...\")\n",
    "            return\n",
    "        \n",
    "        y_train = pd.read_csv(y_train_path)\n",
    "        \n",
    "        # Create a list to collect all tasks\n",
    "        all_tasks = []\n",
    "        \n",
    "        # Process each outlier technique\n",
    "        for outlier_technique in outlier_techniques:\n",
    "            # Process feature added datasets\n",
    "            feature_added_path = os.path.join(\n",
    "                base_dir, \n",
    "                \"Data\", \n",
    "                \"deduplicated_datasets\", \n",
    "                dedup_approach, \n",
    "                \"outlier_handled_datasets\", \n",
    "                outlier_technique, \n",
    "                \"feature_added_datasets\"\n",
    "            )\n",
    "            \n",
    "            if os.path.exists(feature_added_path):\n",
    "                # Use the X_train file from the feature_added_datasets folder\n",
    "                x_train_feature_added = f\"phase2_{dedup_approach}_X_train_{outlier_technique}_feature_added.csv\"\n",
    "                x_train_feature_path = os.path.join(feature_added_path, x_train_feature_added)\n",
    "                \n",
    "                if os.path.exists(x_train_feature_path):\n",
    "                    x_train_feature = pd.read_csv(x_train_feature_path)\n",
    "                    \n",
    "                    # Add balancing tasks for feature added datasets\n",
    "                    for balancing_technique in balancing_techniques:\n",
    "                        all_tasks.append((\n",
    "                            dedup_approach, \n",
    "                            balancing_technique, \n",
    "                            x_train_feature, \n",
    "                            y_train, \n",
    "                            feature_added_path,\n",
    "                            x_train_feature_added\n",
    "                        ))\n",
    "            \n",
    "            # Process each scaling technique\n",
    "            scaled_datasets_path = os.path.join(\n",
    "                base_dir, \n",
    "                \"Data\", \n",
    "                \"deduplicated_datasets\", \n",
    "                dedup_approach, \n",
    "                \"outlier_handled_datasets\", \n",
    "                outlier_technique, \n",
    "                \"scaled_datasets\"\n",
    "            )\n",
    "            \n",
    "            for scaling_technique in scaling_techniques:\n",
    "                scale_folder = os.path.join(scaled_datasets_path, scaling_technique)\n",
    "                \n",
    "                if os.path.exists(scale_folder):\n",
    "                    # Use the X_train file specific to this scaling technique\n",
    "                    x_train_scaled = f\"phase2_{dedup_approach}_X_train_{outlier_technique}_{scaling_technique}.csv\"\n",
    "                    x_train_scaled_path = os.path.join(scale_folder, x_train_scaled)\n",
    "                    \n",
    "                    if os.path.exists(x_train_scaled_path):\n",
    "                        x_train_scaled_data = pd.read_csv(x_train_scaled_path)\n",
    "                        \n",
    "                        # Add balancing tasks for scaled datasets\n",
    "                        for balancing_technique in balancing_techniques:\n",
    "                            all_tasks.append((\n",
    "                                dedup_approach, \n",
    "                                balancing_technique, \n",
    "                                x_train_scaled_data, \n",
    "                                y_train, \n",
    "                                scale_folder,\n",
    "                                x_train_scaled\n",
    "                            ))\n",
    "        \n",
    "        # Process all tasks in parallel\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "            results = list(executor.map(apply_balancing_technique, all_tasks))\n",
    "        \n",
    "        print(f\"Completed processing for {dedup_approach}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dedup approach {dedup_approach}: {str(e)}\")\n",
    "\n",
    "def apply_balancing_techniques_parallel():\n",
    "    \"\"\"Apply balancing techniques to y_train files with corresponding X_train files in parallel\"\"\"\n",
    "    print(\"Starting balancing techniques application in parallel...\")\n",
    "    \n",
    "    # Create overall checkpoint file to track progress\n",
    "    overall_checkpoint = os.path.join(cache_dir, \"overall_progress.txt\")\n",
    "    \n",
    "    # Check if there's a previous run that was interrupted\n",
    "    completed_approaches = set()\n",
    "    if os.path.exists(overall_checkpoint):\n",
    "        with open(overall_checkpoint, 'r') as f:\n",
    "            completed_approaches = set(line.strip() for line in f.readlines())\n",
    "    \n",
    "    # Process each deduplication approach in parallel\n",
    "    with Pool(processes=min(os.cpu_count(), len(dedup_approaches))) as pool:\n",
    "        remaining_approaches = [da for da in dedup_approaches if da not in completed_approaches]\n",
    "        pool.map(process_dedup_approach, remaining_approaches)\n",
    "    \n",
    "    # Update overall checkpoint file\n",
    "    with open(overall_checkpoint, 'w') as f:\n",
    "        for approach in dedup_approaches:\n",
    "            f.write(f\"{approach}\\n\")\n",
    "    \n",
    "    print(\"All balancing techniques applied!\")\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Apply balancing techniques in parallel\n",
    "    apply_balancing_techniques_parallel()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nBalancing techniques applied and files distributed! Total time: {total_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e52a9875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "SYSTEM INFORMATION\n",
      "==================================================\n",
      "CPU: Intel64 Family 6 Model 154 Stepping 4, GenuineIntel\n",
      "CPU Cores: 12 physical cores\n",
      "CPU Logical Processors: 12 logical processors\n",
      "CPU Frequency: 1300.00 MHz\n",
      "Total Memory: 7.68 GB\n",
      "Available Memory: 0.32 GB\n",
      "Disk Total: 476.34 GB\n",
      "Disk Free: 255.17 GB\n",
      "\n",
      "Disk I/O Speed Test:\n",
      "Write Speed: 321.63 MB/s\n",
      "Read Speed: 305.06 MB/s\n",
      "Storage Type: Likely SSD (Fast)\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "BALANCING PERFORMANCE BENCHMARK\n",
      "==================================================\n",
      "Creating synthetic data with shape X: [626043, 40], y: [671698, 1]\n",
      "Running SMOTETomek on 10000 samples...\n",
      "Sample SMOTETomek completed in 0.91 seconds\n",
      "Estimated SMOTETomek time for full dataset: 3555.30 seconds (59.26 minutes)\n",
      "\n",
      "Testing pandas DataFrame I/O speed...\n",
      "Pandas CSV write time for 10000 rows: 0.88 seconds\n",
      "Pandas CSV read time for 10000 rows: 0.19 seconds\n",
      "Estimated I/O time for all operations: 25142.35 seconds (419.04 minutes)\n",
      "\n",
      "ESTIMATED TOTAL RUNTIME:\n",
      "Without parallelization: 81.05 hours\n",
      "With parallelization (12 cores): 8.11 hours\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import psutil\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import multiprocessing\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "def get_system_info():\n",
    "    \"\"\"Get detailed system information\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"SYSTEM INFORMATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # CPU information\n",
    "    print(f\"CPU: {platform.processor()}\")\n",
    "    print(f\"CPU Cores: {multiprocessing.cpu_count()} physical cores\")\n",
    "    print(f\"CPU Logical Processors: {psutil.cpu_count(logical=True)} logical processors\")\n",
    "    print(f\"CPU Frequency: {psutil.cpu_freq().current:.2f} MHz\")\n",
    "    \n",
    "    # Memory information\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"Total Memory: {mem.total / (1024**3):.2f} GB\")\n",
    "    print(f\"Available Memory: {mem.available / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # Disk information\n",
    "    disk = psutil.disk_usage('/')\n",
    "    print(f\"Disk Total: {disk.total / (1024**3):.2f} GB\")\n",
    "    print(f\"Disk Free: {disk.free / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # Check if running on SSD or HDD\n",
    "    print(\"\\nDisk I/O Speed Test:\")\n",
    "    test_disk_speed()\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "def test_disk_speed(file_size_mb=100):\n",
    "    \"\"\"Test disk read/write speed\"\"\"\n",
    "    # Create a temporary directory\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    test_file = os.path.join(temp_dir, \"test_file.bin\")\n",
    "    \n",
    "    try:\n",
    "        # Write test\n",
    "        data = b\"0\" * (file_size_mb * 1024 * 1024)  # Create file_size_mb of data\n",
    "        start_time = time.time()\n",
    "        with open(test_file, 'wb') as f:\n",
    "            f.write(data)\n",
    "        write_time = time.time() - start_time\n",
    "        write_speed = file_size_mb / write_time\n",
    "        \n",
    "        # Read test\n",
    "        start_time = time.time()\n",
    "        with open(test_file, 'rb') as f:\n",
    "            data = f.read()\n",
    "        read_time = time.time() - start_time\n",
    "        read_speed = file_size_mb / read_time\n",
    "        \n",
    "        print(f\"Write Speed: {write_speed:.2f} MB/s\")\n",
    "        print(f\"Read Speed: {read_speed:.2f} MB/s\")\n",
    "        \n",
    "        # Classify disk type based on read speed\n",
    "        if read_speed > 200:\n",
    "            print(\"Storage Type: Likely SSD (Fast)\")\n",
    "        elif read_speed > 80:\n",
    "            print(\"Storage Type: Likely Fast HDD or Hybrid\")\n",
    "        else:\n",
    "            print(\"Storage Type: Likely HDD (Slower)\")\n",
    "            \n",
    "    finally:\n",
    "        # Clean up\n",
    "        shutil.rmtree(temp_dir)\n",
    "\n",
    "def benchmark_smote_tomek(x_size, y_size, n_features=40):\n",
    "    \"\"\"Benchmark SMOTETomek performance on sample data\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"BALANCING PERFORMANCE BENCHMARK\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create synthetic data with similar dimensions\n",
    "    print(f\"Creating synthetic data with shape X: [{x_size}, {n_features}], y: [{y_size}, 1]\")\n",
    "    X = np.random.random((x_size, n_features))\n",
    "    y = np.random.choice([0, 1], size=y_size, p=[0.7, 0.3])  # Create imbalanced data\n",
    "    \n",
    "    # Sample subset for quick benchmark\n",
    "    sample_size = min(10000, x_size)\n",
    "    X_sample = X[:sample_size]\n",
    "    y_sample = y[:sample_size]\n",
    "    \n",
    "    # Test SMOTETomek on sample\n",
    "    print(f\"Running SMOTETomek on {sample_size} samples...\")\n",
    "    start_time = time.time()\n",
    "    smote_tomek = SMOTETomek(random_state=42, n_jobs=-1)\n",
    "    X_res, y_res = smote_tomek.fit_resample(X_sample, y_sample)\n",
    "    sample_time = time.time() - start_time\n",
    "    \n",
    "    # Estimate full runtime\n",
    "    estimated_full_time = sample_time * (x_size / sample_size)**2  # SMOTETomek scales roughly quadratically\n",
    "    \n",
    "    print(f\"Sample SMOTETomek completed in {sample_time:.2f} seconds\")\n",
    "    print(f\"Estimated SMOTETomek time for full dataset: {estimated_full_time:.2f} seconds ({estimated_full_time/60:.2f} minutes)\")\n",
    "    \n",
    "    # Test file I/O with pandas\n",
    "    print(\"\\nTesting pandas DataFrame I/O speed...\")\n",
    "    df = pd.DataFrame(X_sample)\n",
    "    \n",
    "    # Test write speed\n",
    "    start_time = time.time()\n",
    "    with tempfile.NamedTemporaryFile(suffix='.csv', delete=False) as tmp:\n",
    "        tmp_name = tmp.name\n",
    "    df.to_csv(tmp_name, index=False)\n",
    "    write_time = time.time() - start_time\n",
    "    \n",
    "    # Test read speed\n",
    "    start_time = time.time()\n",
    "    pd.read_csv(tmp_name)\n",
    "    read_time = time.time() - start_time\n",
    "    \n",
    "    # Clean up\n",
    "    os.remove(tmp_name)\n",
    "    \n",
    "    print(f\"Pandas CSV write time for {sample_size} rows: {write_time:.2f} seconds\")\n",
    "    print(f\"Pandas CSV read time for {sample_size} rows: {read_time:.2f} seconds\")\n",
    "    \n",
    "    # Estimate total data processing time\n",
    "    total_operations = 3 * 5 * 5 * 5  # dedup_approaches * outlier_techniques * scaling_techniques * balancing_techniques\n",
    "    estimated_io_time = (write_time + read_time) * (x_size / sample_size) * total_operations\n",
    "    \n",
    "    print(f\"Estimated I/O time for all operations: {estimated_io_time:.2f} seconds ({estimated_io_time/60:.2f} minutes)\")\n",
    "    \n",
    "    # Total estimated runtime\n",
    "    print(\"\\nESTIMATED TOTAL RUNTIME:\")\n",
    "    serial_runtime = estimated_full_time * total_operations / 5 + estimated_io_time  # Divide by 5 because not all balancing techniques are as slow as SMOTETomek\n",
    "    parallel_runtime = serial_runtime / psutil.cpu_count(logical=True) * 1.2  # Add 20% overhead for parallel coordination\n",
    "    \n",
    "    print(f\"Without parallelization: {serial_runtime/3600:.2f} hours\")\n",
    "    print(f\"With parallelization ({psutil.cpu_count(logical=True)} cores): {parallel_runtime/3600:.2f} hours\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "def main():\n",
    "    # Get system information\n",
    "    get_system_info()\n",
    "    \n",
    "    # Run benchmark with your data dimensions\n",
    "    benchmark_smote_tomek(626043, 671698, 40)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c096fc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from imbalanced-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from imbalanced-learn) (1.6.1)\n",
      "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn)\n",
      "  Downloading sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "Downloading imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
      "Downloading sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: sklearn-compat, imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.13.0 sklearn-compat-0.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: C:\\Users\\HP\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

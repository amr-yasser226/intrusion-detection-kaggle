{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8676e569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phase2_Direct_Removal_X_train.csv in Direct_Removal...\n",
      "  - Saved Winsorization result to c:\\Machine Learning\\Phase 2\\Data\\deduplicated_datasets\\Direct_Removal\\phase2_Direct_Removal_X_train_Winsorization.csv\n",
      "  - Saved Direct Removal result to c:\\Machine Learning\\Phase 2\\Data\\deduplicated_datasets\\Direct_Removal\\phase2_Direct_Removal_X_train_DirectRemoval.csv\n",
      "  - Saved Z-Score Trimming result to c:\\Machine Learning\\Phase 2\\Data\\deduplicated_datasets\\Direct_Removal\\phase2_Direct_Removal_X_train_ZScoreTrimming.csv\n",
      "  - Saved Log1p+Winsorization result to c:\\Machine Learning\\Phase 2\\Data\\deduplicated_datasets\\Direct_Removal\\phase2_Direct_Removal_X_train_Log1pWinsorization.csv\n",
      "  - Saved Isolation Forest result to c:\\Machine Learning\\Phase 2\\Data\\deduplicated_datasets\\Direct_Removal\\phase2_Direct_Removal_X_train_IsolationForest.csv\n",
      "Processing phase2_Instance_Weighting_X_train.csv in Instance_Weighting...\n",
      "  - Saved Winsorization result to c:\\Machine Learning\\Phase 2\\Data\\deduplicated_datasets\\Instance_Weighting\\phase2_Instance_Weighting_X_train_Winsorization.csv\n",
      "  - Saved Direct Removal result to c:\\Machine Learning\\Phase 2\\Data\\deduplicated_datasets\\Instance_Weighting\\phase2_Instance_Weighting_X_train_DirectRemoval.csv\n",
      "  - Saved Z-Score Trimming result to c:\\Machine Learning\\Phase 2\\Data\\deduplicated_datasets\\Instance_Weighting\\phase2_Instance_Weighting_X_train_ZScoreTrimming.csv\n",
      "  - Saved Log1p+Winsorization result to c:\\Machine Learning\\Phase 2\\Data\\deduplicated_datasets\\Instance_Weighting\\phase2_Instance_Weighting_X_train_Log1pWinsorization.csv\n",
      "  - Saved Isolation Forest result to c:\\Machine Learning\\Phase 2\\Data\\deduplicated_datasets\\Instance_Weighting\\phase2_Instance_Weighting_X_train_IsolationForest.csv\n",
      "Processing phase2_TrainTestAware_X_train.csv in Train_Test_Aware...\n",
      "  - Saved Winsorization result to c:\\Machine Learning\\Phase 2\\Data\\deduplicated_datasets\\Train_Test_Aware\\phase2_TrainTestAware_X_train_Winsorization.csv\n",
      "  - Saved Direct Removal result to c:\\Machine Learning\\Phase 2\\Data\\deduplicated_datasets\\Train_Test_Aware\\phase2_TrainTestAware_X_train_DirectRemoval.csv\n",
      "  - Saved Z-Score Trimming result to c:\\Machine Learning\\Phase 2\\Data\\deduplicated_datasets\\Train_Test_Aware\\phase2_TrainTestAware_X_train_ZScoreTrimming.csv\n",
      "  - Saved Log1p+Winsorization result to c:\\Machine Learning\\Phase 2\\Data\\deduplicated_datasets\\Train_Test_Aware\\phase2_TrainTestAware_X_train_Log1pWinsorization.csv\n",
      "  - Saved Isolation Forest result to c:\\Machine Learning\\Phase 2\\Data\\deduplicated_datasets\\Train_Test_Aware\\phase2_TrainTestAware_X_train_IsolationForest.csv\n",
      "Outlier handling completed for all X_train files!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Define the columns to process for outlier handling\n",
    "OUTLIER_COLUMNS = [\n",
    "    'flow_time', 'header_size', 'packet_duration', 'overall_rate', \n",
    "    'src_rate', 'dst_rate', 'fin_packets', 'urg_packets', 'rst_packets', \n",
    "    'max_value', 'value_covariance'\n",
    "]\n",
    "\n",
    "# Define the base directory where data is stored\n",
    "BASE_DIR = os.path.join(os.getcwd(), 'Data', 'deduplicated_datasets')\n",
    "\n",
    "# Define the techniques to be applied\n",
    "TECHNIQUES = [\n",
    "    'Winsorization',\n",
    "    'DirectRemoval',\n",
    "    'ZScoreTrimming',\n",
    "    'Log1pWinsorization',\n",
    "    'IsolationForest'\n",
    "]\n",
    "\n",
    "def winsorization(df, columns, lower_percentile=0.05, upper_percentile=0.95):\n",
    "    \"\"\"Apply winsorization (percentile-based capping) to specified columns\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    for col in columns:\n",
    "        if col in df_copy.columns:\n",
    "            lower_bound = df_copy[col].quantile(lower_percentile)\n",
    "            upper_bound = df_copy[col].quantile(upper_percentile)\n",
    "            df_copy[col] = df_copy[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "    return df_copy\n",
    "\n",
    "def direct_removal(df, columns, lower_percentile=0.01, upper_percentile=0.99):\n",
    "    \"\"\"Remove outliers directly based on percentiles\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    mask = pd.Series(True, index=df_copy.index)\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df_copy.columns:\n",
    "            lower_bound = df_copy[col].quantile(lower_percentile)\n",
    "            upper_bound = df_copy[col].quantile(upper_percentile)\n",
    "            col_mask = (df_copy[col] >= lower_bound) & (df_copy[col] <= upper_bound)\n",
    "            mask = mask & col_mask\n",
    "    \n",
    "    return df_copy[mask]\n",
    "\n",
    "def zscore_trimming(df, columns, threshold=3):\n",
    "    \"\"\"Apply Z-score trimming to remove outliers\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    mask = pd.Series(True, index=df_copy.index)\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df_copy.columns:\n",
    "            z_scores = np.abs(stats.zscore(df_copy[col], nan_policy='omit'))\n",
    "            col_mask = z_scores < threshold\n",
    "            mask = mask & col_mask\n",
    "    \n",
    "    return df_copy[mask]\n",
    "\n",
    "def log1p_winsorization(df, columns, lower_percentile=0.05, upper_percentile=0.95):\n",
    "    \"\"\"Apply log1p transformation followed by winsorization\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    for col in columns:\n",
    "        if col in df_copy.columns:\n",
    "            # Apply log1p transformation\n",
    "            df_copy[col] = np.log1p(df_copy[col].clip(lower=0))  # Ensuring non-negative values\n",
    "            # Apply winsorization\n",
    "            lower_bound = df_copy[col].quantile(lower_percentile)\n",
    "            upper_bound = df_copy[col].quantile(upper_percentile)\n",
    "            df_copy[col] = df_copy[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "    return df_copy\n",
    "\n",
    "def isolation_forest_filtering(df, columns, contamination=0.05):\n",
    "    \"\"\"Use Isolation Forest to identify and remove outliers\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Only use the specified columns for outlier detection\n",
    "    cols_to_use = [col for col in columns if col in df_copy.columns]\n",
    "    if not cols_to_use:\n",
    "        return df_copy\n",
    "    \n",
    "    # Handle NaN values\n",
    "    subset_df = df_copy[cols_to_use].fillna(df_copy[cols_to_use].mean())\n",
    "    \n",
    "    # Apply Isolation Forest\n",
    "    model = IsolationForest(contamination=contamination, random_state=42)\n",
    "    outlier_predictions = model.fit_predict(subset_df)\n",
    "    \n",
    "    # Keep only inliers (1 is inlier, -1 is outlier)\n",
    "    return df_copy[outlier_predictions == 1]\n",
    "\n",
    "def process_x_train_files():\n",
    "    \"\"\"Process each X_train file in the deduplicated datasets folders\"\"\"\n",
    "    # Find all the deduplication technique folders\n",
    "    technique_folders = [folder for folder in os.listdir(BASE_DIR) if os.path.isdir(os.path.join(BASE_DIR, folder))]\n",
    "    \n",
    "    for folder in technique_folders:\n",
    "        folder_path = os.path.join(BASE_DIR, folder)\n",
    "        \n",
    "        # Find the X_train file in this folder\n",
    "        for file in os.listdir(folder_path):\n",
    "            if \"X_train\" in file and file.endswith(\".csv\"):\n",
    "                input_file_path = os.path.join(folder_path, file)\n",
    "                \n",
    "                # Load the dataset\n",
    "                df = pd.read_csv(input_file_path)\n",
    "                \n",
    "                # Base name for output files\n",
    "                file_prefix = file.replace('.csv', '')\n",
    "                \n",
    "                # Apply each outlier handling technique\n",
    "                print(f\"Processing {file} in {folder}...\")\n",
    "                \n",
    "                # 1. Winsorization\n",
    "                winsorized_df = winsorization(df, OUTLIER_COLUMNS)\n",
    "                winsorized_output_path = os.path.join(folder_path, f\"{file_prefix}_Winsorization.csv\")\n",
    "                winsorized_df.to_csv(winsorized_output_path, index=False)\n",
    "                print(f\"  - Saved Winsorization result to {winsorized_output_path}\")\n",
    "                \n",
    "                # 2. Direct Removal\n",
    "                direct_removal_df = direct_removal(df, OUTLIER_COLUMNS)\n",
    "                direct_removal_output_path = os.path.join(folder_path, f\"{file_prefix}_DirectRemoval.csv\")\n",
    "                direct_removal_df.to_csv(direct_removal_output_path, index=False)\n",
    "                print(f\"  - Saved Direct Removal result to {direct_removal_output_path}\")\n",
    "                \n",
    "                # 3. Z-Score Trimming\n",
    "                zscore_df = zscore_trimming(df, OUTLIER_COLUMNS)\n",
    "                zscore_output_path = os.path.join(folder_path, f\"{file_prefix}_ZScoreTrimming.csv\")\n",
    "                zscore_df.to_csv(zscore_output_path, index=False)\n",
    "                print(f\"  - Saved Z-Score Trimming result to {zscore_output_path}\")\n",
    "                \n",
    "                # 4. Log1p + Winsorization\n",
    "                log_win_df = log1p_winsorization(df, OUTLIER_COLUMNS)\n",
    "                log_win_output_path = os.path.join(folder_path, f\"{file_prefix}_Log1pWinsorization.csv\")\n",
    "                log_win_df.to_csv(log_win_output_path, index=False)\n",
    "                print(f\"  - Saved Log1p+Winsorization result to {log_win_output_path}\")\n",
    "                \n",
    "                # 5. Isolation Forest\n",
    "                iso_forest_df = isolation_forest_filtering(df, OUTLIER_COLUMNS)\n",
    "                iso_forest_output_path = os.path.join(folder_path, f\"{file_prefix}_IsolationForest.csv\")\n",
    "                iso_forest_df.to_csv(iso_forest_output_path, index=False)\n",
    "                print(f\"  - Saved Isolation Forest result to {iso_forest_output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_x_train_files()\n",
    "    print(\"Outlier handling completed for all X_train files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf0191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, StackingClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from joblib import dump, load\n",
    "\n",
    "# Numeric features\n",
    "num_cols = [\n",
    "    \"flow_time\", \"header_size\", \"packet_duration\", \"overall_rate\",\n",
    "    \"src_rate\", \"dst_rate\", \"fin_packets\", \"urg_packets\",\n",
    "    \"rst_packets\", \"max_value\", \"value_covariance\"\n",
    "]\n",
    "\n",
    "def load_and_clean(path):\n",
    "    \"\"\"Load, dedupe, shuffle, winsorize, and log-transform numeric columns.\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.drop_duplicates().sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    for col in num_cols:\n",
    "        lo, hi = np.percentile(df[col], [1, 99])\n",
    "        df[col] = df[col].clip(lo, hi)\n",
    "        df[col] = np.log1p(df[col])\n",
    "    return df\n",
    "\n",
    "\n",
    "def fit_transformers(X, y):\n",
    "    \"\"\"Fit label encoder, scaler, and selector; dump them for later reuse.\"\"\"\n",
    "    le = LabelEncoder().fit(y)\n",
    "    y_enc = le.transform(y)\n",
    "    scaler = StandardScaler().fit(X[num_cols])\n",
    "    X[num_cols] = scaler.transform(X[num_cols])\n",
    "    selector = ExtraTreesClassifier(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    selector.fit(X, y_enc)\n",
    "    sfm = SelectFromModel(selector, prefit=True, threshold=\"median\")\n",
    "    X_sel = sfm.transform(X)\n",
    "    dump(le, 'le.joblib')\n",
    "    dump(scaler, 'scaler.joblib')\n",
    "    dump(sfm, 'selector.joblib')\n",
    "    return X_sel, y_enc, le.classes_\n",
    "\n",
    "\n",
    "def apply_transformers(df):\n",
    "    \"\"\"Load and apply saved transformers to new data.\"\"\"\n",
    "    le = load('le.joblib')\n",
    "    scaler = load('scaler.joblib')\n",
    "    sfm = load('selector.joblib')\n",
    "    X = df.drop(columns=['label'])\n",
    "    y = le.transform(df['label'].values)\n",
    "    X[num_cols] = scaler.transform(X[num_cols])\n",
    "    X_sel = sfm.transform(X)\n",
    "    return X_sel, y, le.classes_\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    df = load_and_clean(args.train_data)\n",
    "    X = df.drop(columns=['label'])\n",
    "    y = df['label'].values\n",
    "    X_sel, y_enc, class_names = fit_transformers(X.copy(), y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_sel, y_enc, test_size=0.2, stratify=y_enc, random_state=42\n",
    "    )\n",
    "    # Train XGBoost\n",
    "    xgb = XGBClassifier(n_estimators=100, max_depth=6,\n",
    "                        use_label_encoder=False, eval_metric='mlogloss',\n",
    "                        random_state=42, n_jobs=-1)\n",
    "    print(\"Training XGBoost...\")\n",
    "    xgb.fit(X_train, y_train)\n",
    "    dump(xgb, 'xgb_model.joblib')\n",
    "    preds = xgb.predict(X_test)\n",
    "    print(f\"XGBoost accuracy: {accuracy_score(y_test, preds):.4f}\")\n",
    "    print(classification_report(y_test, preds, target_names=class_names))\n",
    "    # Train Stacking\n",
    "    base_estimators = [\n",
    "        ('dt', DecisionTreeClassifier(max_depth=10, class_weight='balanced', random_state=42)),\n",
    "        ('et', ExtraTreesClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)),\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, max_depth=10, class_weight='balanced', random_state=42, n_jobs=-1)),\n",
    "        ('brf', BalancedRandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)),\n",
    "        ('xgb', XGBClassifier(n_estimators=100, max_depth=6, use_label_encoder=False, eval_metric='mlogloss', random_state=42, n_jobs=-1))\n",
    "    ]\n",
    "    stack = StackingClassifier(estimators=base_estimators,\n",
    "                               final_estimator=DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "                               stack_method='predict_proba', n_jobs=-1)\n",
    "    print(\"Training Stacking Ensemble...\")\n",
    "    stack.fit(X_train, y_train)\n",
    "    dump(stack, 'stacking_model.joblib')\n",
    "    preds = stack.predict(X_test)\n",
    "    print(f\"Stacking accuracy: {accuracy_score(y_test, preds):.4f}\")\n",
    "    print(classification_report(y_test, preds, target_names=class_names))\n",
    "\n",
    "\n",
    "def test(args):\n",
    "    # Ensure models and transformers exist\n",
    "    required = ['xgb_model.joblib', 'stacking_model.joblib', 'le.joblib', 'scaler.joblib', 'selector.joblib']\n",
    "    if not all(os.path.exists(f) for f in required):\n",
    "        raise FileNotFoundError(\"Missing files: run in train mode first.\")\n",
    "    df_test = load_and_clean(args.test_data)\n",
    "    X_test, y_test, class_names = apply_transformers(df_test)\n",
    "    for name, path in [('XGBoost', 'xgb_model.joblib'), ('Stacking', 'stacking_model.joblib')]:\n",
    "        model = load(path)\n",
    "        preds = model.predict(X_test)\n",
    "        print(f\"{name} accuracy on {os.path.basename(args.test_data)}: {accuracy_score(y_test, preds):.4f}\")\n",
    "        print(classification_report(y_test, preds, target_names=class_names))\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    sub = parser.add_subparsers(dest='mode')  # allow unknown args\n",
    "    tr = sub.add_parser('train')\n",
    "    tr.add_argument('--train-data', required=True)\n",
    "    te = sub.add_parser('test')\n",
    "    te.add_argument('--test-data', required=True)\n",
    "    # ignore notebook args\n",
    "    args, _ = parser.parse_known_args()\n",
    "    if args.mode == 'train':\n",
    "        train(args)\n",
    "    elif args.mode == 'test':\n",
    "        test(args)\n",
    "    else:\n",
    "        parser.print_help()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6728eaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test rows: 104287\n",
      "predictions: 168480\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array length 168480 does not match index length 104287",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m preds_str \u001b[38;5;241m=\u001b[39m [ num2label[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m preds_num ]\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# build submission DataFrame\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m sub \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m     \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds_str\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# save\u001b[39;00m\n\u001b[0;32m     45\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_submission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\internals\\construction.py:690\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lengths[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m    686\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    687\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlengths[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    688\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    689\u001b[0m         )\n\u001b[1;32m--> 690\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    692\u001b[0m     index \u001b[38;5;241m=\u001b[39m default_index(lengths[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: array length 168480 does not match index length 104287"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from joblib import load\n",
    "\n",
    "# 1. Load your test set (must contain an 'Id' column)\n",
    "test_df = pd.read_csv('C:\\\\Users\\\\HP\\\\Downloads\\\\test.csv')\n",
    "\n",
    "# 2. Define the numeric→string mapping (invert of your label encoder)\n",
    "num2label = {\n",
    "    0: 'BenignTraffic',\n",
    "    1: 'DDoS',\n",
    "    2: 'DoS',\n",
    "    3: 'MITM',\n",
    "    4: 'Mirai',\n",
    "    5: 'Recon'\n",
    "}\n",
    "\n",
    "# 3. List all your .joblib prediction files\n",
    "prediction_files = {\n",
    "    'xgb':      'C:\\\\Users\\\\HP\\\\Downloads\\\\xgb_preds.joblib',\n",
    "    'stacking': 'C:\\\\Users\\\\HP\\\\Downloads\\\\stacking_preds.joblib'\n",
    "}\n",
    "\n",
    "# Immediately before building the DataFrame, add:\n",
    "print(\"test rows:\",   test_df.shape[0])\n",
    "print(\"predictions:\", len(preds_num))\n",
    "\n",
    "# That will pin down whether it's your CSV or your preds array that’s “wrong”.\n",
    "\n",
    "\n",
    "# 4. For each, load the numeric preds, map → strings, and save a submission CSV\n",
    "for model_name, joblib_path in prediction_files.items():\n",
    "    # load the array of numeric codes\n",
    "    preds_num = load(joblib_path)\n",
    "    \n",
    "    # map to strings\n",
    "    preds_str = [ num2label[i] for i in preds_num ]\n",
    "    \n",
    "    # build submission DataFrame\n",
    "    sub = pd.DataFrame({\n",
    "        'Id':     test_df['Id'],\n",
    "        'Target': preds_str\n",
    "    })\n",
    "    \n",
    "    # save\n",
    "    filename = f'{model_name}_submission.csv'\n",
    "    sub.to_csv(filename, index=False)\n",
    "    print(f'Wrote {filename} ({len(sub)} rows)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eac39ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded with shape: (938583, 22)\n",
      "\n",
      "--- Data Exploration ---\n",
      "Sample of data:\n",
      "   flow_time  header_size  packet_duration  overall_rate     src_rate  \\\n",
      "0   0.041268     15499.00            64.00   7805.845961  7805.845961   \n",
      "1   0.018393      3702.54            64.00   6728.994198  6728.994198   \n",
      "2   0.000000       182.00            64.00     38.559448    38.559448   \n",
      "3   0.109292     35027.55            62.72   6783.234241  6783.234241   \n",
      "4   0.000000       162.00            64.00      2.305494     2.305494   \n",
      "\n",
      "   dst_rate  fin_packets  urg_packets  rst_packets  max_value  ...  syn_flags  \\\n",
      "0       0.0          0.0         0.00         0.00      50.00  ...          0   \n",
      "1       0.0          0.0         0.00         0.01      54.28  ...          0   \n",
      "2       0.0          0.0         0.00         0.00     182.00  ...          0   \n",
      "3       0.0          0.0         0.03         0.11      65.11  ...          0   \n",
      "4       0.0          0.0         0.00         0.00     162.00  ...          0   \n",
      "\n",
      "   rst_flags  psh_flags  ack_flags  protocol_http  protocol_https  \\\n",
      "0          0          0          0              0               0   \n",
      "1          0          0          0              0               0   \n",
      "2          0          0          0              0               0   \n",
      "3          0          0          0              0               0   \n",
      "4          0          0          0              0               0   \n",
      "\n",
      "   protocol_tcp  protocol_udp  protocol_icmp  label  \n",
      "0             0             1              0   DDoS  \n",
      "1             0             1              0   DDoS  \n",
      "2             0             1              0    DoS  \n",
      "3             0             1              0    DoS  \n",
      "4             0             1              0    DoS  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "\n",
      "Data information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 938583 entries, 0 to 938582\n",
      "Data columns (total 22 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   flow_time         938583 non-null  float64\n",
      " 1   header_size       938583 non-null  float64\n",
      " 2   packet_duration   938583 non-null  float64\n",
      " 3   overall_rate      938583 non-null  float64\n",
      " 4   src_rate          938583 non-null  float64\n",
      " 5   dst_rate          938583 non-null  float64\n",
      " 6   fin_packets       938583 non-null  float64\n",
      " 7   urg_packets       938583 non-null  float64\n",
      " 8   rst_packets       938583 non-null  float64\n",
      " 9   max_value         938583 non-null  float64\n",
      " 10  value_covariance  938583 non-null  float64\n",
      " 11  fin_flags         938583 non-null  int64  \n",
      " 12  syn_flags         938583 non-null  int64  \n",
      " 13  rst_flags         938583 non-null  int64  \n",
      " 14  psh_flags         938583 non-null  int64  \n",
      " 15  ack_flags         938583 non-null  int64  \n",
      " 16  protocol_http     938583 non-null  int64  \n",
      " 17  protocol_https    938583 non-null  int64  \n",
      " 18  protocol_tcp      938583 non-null  int64  \n",
      " 19  protocol_udp      938583 non-null  int64  \n",
      " 20  protocol_icmp     938583 non-null  int64  \n",
      " 21  label             938583 non-null  object \n",
      "dtypes: float64(11), int64(10), object(1)\n",
      "memory usage: 157.5+ MB\n",
      "None\n",
      "\n",
      "Basic statistics:\n",
      "          flow_time   header_size  packet_duration  overall_rate  \\\n",
      "count  9.385830e+05  9.385830e+05    938583.000000  9.385830e+05   \n",
      "mean   1.342515e+01  1.018134e+05        82.664589  9.163496e+03   \n",
      "std    5.898099e+03  1.801377e+06       166.986325  1.001806e+05   \n",
      "min    0.000000e+00  0.000000e+00         0.000000  0.000000e+00   \n",
      "25%    0.000000e+00  5.400000e+01        64.000000  2.077046e+00   \n",
      "50%    0.000000e+00  5.400000e+01        64.000000  1.570377e+01   \n",
      "75%    1.017542e-01  3.640000e+02        64.000000  1.177706e+02   \n",
      "max    4.930147e+06  3.311174e+08      6525.740000  7.340032e+06   \n",
      "\n",
      "           src_rate       dst_rate    fin_packets    urg_packets  \\\n",
      "count  9.385830e+05  938583.000000  938583.000000  938583.000000   \n",
      "mean   9.163496e+03       0.000002       0.099474       5.850813   \n",
      "std    1.001806e+05       0.000898       0.299712      70.715367   \n",
      "min    0.000000e+00       0.000000       0.000000       0.000000   \n",
      "25%    2.077046e+00       0.000000       0.000000       0.000000   \n",
      "50%    1.570377e+01       0.000000       0.000000       0.000000   \n",
      "75%    1.177706e+02       0.000000       0.000000       0.000000   \n",
      "max    7.340032e+06       0.848465      19.500000    4136.700000   \n",
      "\n",
      "         rst_packets      max_value  ...      fin_flags      syn_flags  \\\n",
      "count  938583.000000  938583.000000  ...  938583.000000  938583.000000   \n",
      "mean       37.137514     177.161360  ...       0.087102       0.208346   \n",
      "std       324.613580     515.425327  ...       0.281984       0.406126   \n",
      "min         0.000000      42.000000  ...       0.000000       0.000000   \n",
      "25%         0.000000      50.000000  ...       0.000000       0.000000   \n",
      "50%         0.000000      54.000000  ...       0.000000       0.000000   \n",
      "75%         0.010000      55.140000  ...       0.000000       0.000000   \n",
      "max      9331.500000   30329.200000  ...       1.000000       1.000000   \n",
      "\n",
      "           rst_flags      psh_flags      ack_flags  protocol_http  \\\n",
      "count  938583.000000  938583.000000  938583.000000  938583.000000   \n",
      "mean        0.091149       0.088415       0.121502       0.048001   \n",
      "std         0.287821       0.283898       0.326710       0.213769   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         0.000000       0.000000       0.000000       0.000000   \n",
      "75%         0.000000       0.000000       0.000000       0.000000   \n",
      "max         1.000000       1.000000       1.000000       1.000000   \n",
      "\n",
      "       protocol_https   protocol_tcp   protocol_udp  protocol_icmp  \n",
      "count   938583.000000  938583.000000  938583.000000  938583.000000  \n",
      "mean         0.054196       0.574175       0.211410       0.164314  \n",
      "std          0.226403       0.494468       0.408309       0.370560  \n",
      "min          0.000000       0.000000       0.000000       0.000000  \n",
      "25%          0.000000       0.000000       0.000000       0.000000  \n",
      "50%          0.000000       1.000000       0.000000       0.000000  \n",
      "75%          0.000000       1.000000       0.000000       0.000000  \n",
      "max          1.000000       1.000000       1.000000       1.000000  \n",
      "\n",
      "[8 rows x 21 columns]\n",
      "\n",
      "Class distribution:\n",
      "label\n",
      "DDoS             687027\n",
      "DoS              163428\n",
      "Mirai             53395\n",
      "BenignTraffic     21987\n",
      "Recon              6433\n",
      "MITM               6313\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Data Preprocessing ---\n",
      "Missing values:\n",
      "flow_time           0\n",
      "header_size         0\n",
      "packet_duration     0\n",
      "overall_rate        0\n",
      "src_rate            0\n",
      "dst_rate            0\n",
      "fin_packets         0\n",
      "urg_packets         0\n",
      "rst_packets         0\n",
      "max_value           0\n",
      "value_covariance    0\n",
      "fin_flags           0\n",
      "syn_flags           0\n",
      "rst_flags           0\n",
      "psh_flags           0\n",
      "ack_flags           0\n",
      "protocol_http       0\n",
      "protocol_https      0\n",
      "protocol_tcp        0\n",
      "protocol_udp        0\n",
      "protocol_icmp       0\n",
      "label               0\n",
      "dtype: int64\n",
      "Number of duplicates: 96187\n",
      "Duplicates have been removed.\n",
      "\n",
      "Checking for outliers in numerical features...\n",
      "\n",
      "--- Feature Engineering ---\n",
      "Added new feature: rate_ratio (src_rate / dst_rate)\n",
      "Added new feature: control_packet_ratio (control packets per second)\n",
      "\n",
      "Top 10 important features:\n",
      "                 Feature  Importance\n",
      "0              flow_time    0.178521\n",
      "1            header_size    0.138731\n",
      "9              max_value    0.121775\n",
      "3           overall_rate    0.100245\n",
      "4               src_rate    0.095191\n",
      "11            rate_ratio    0.094805\n",
      "22         protocol_icmp    0.047793\n",
      "21          protocol_udp    0.035642\n",
      "10      value_covariance    0.034769\n",
      "12  control_packet_ratio    0.033985\n",
      "\n",
      "--- Data Splitting and Preparation ---\n",
      "Training set shape: (673916, 23), Testing set shape: (168480, 23)\n",
      "\n",
      "Applying SMOTE to handle imbalance for classes: ['Mirai', 'BenignTraffic', 'Recon', 'MITM']\n",
      "Training set shape after SMOTE: (2869902, 23)\n",
      "Class distribution after SMOTE:\n",
      "label\n",
      "DDoS             478317\n",
      "DoS              478317\n",
      "Mirai            478317\n",
      "Recon            478317\n",
      "MITM             478317\n",
      "BenignTraffic    478317\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Model Building and Evaluation ---\n",
      "\n",
      "Training RandomForest...\n",
      "Performing grid search with 3-fold cross-validation (threading backend)...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 7.30 MiB for an array with shape (956634, 1) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\_utils.py\", line 72, in __call__\n    return self.func(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\parallel.py\", line 139, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    X_test, y_test = _safe_split(estimator, X, y, test, train)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\metaestimators.py\", line 156, in _safe_split\n    X_subset = _safe_indexing(X, indices)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_indexing.py\", line 266, in _safe_indexing\n    return _pandas_indexing(X, indices, indices_dtype, axis=axis)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_indexing.py\", line 47, in _pandas_indexing\n    return X.take(key, axis=axis)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\generic.py\", line 4133, in take\n    new_data = self._mgr.take(\n               ^^^^^^^^^^^^^^^\n  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\internals\\managers.py\", line 894, in take\n    return self.reindex_indexer(\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\internals\\managers.py\", line 688, in reindex_indexer\n    blk.take_nd(\n  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\internals\\blocks.py\", line 1307, in take_nd\n    new_values = algos.take_nd(\n                 ^^^^^^^^^^^^^^\n  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\array_algos\\take.py\", line 117, in take_nd\n    return _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\array_algos\\take.py\", line 157, in _take_nd_ndarray\n    out = np.empty(out_shape, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 7.30 MiB for an array with shape (956634, 1) and data type float64\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 292\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBest model saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcyber_attack_classifier.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 292\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 282\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    280\u001b[0m df, numerical_features, binary_features \u001b[38;5;241m=\u001b[39m engineer_features(df, numerical_features, binary_features)\n\u001b[0;32m    281\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m prepare_data_for_modeling(df, numerical_features, binary_features)\n\u001b[1;32m--> 282\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_and_evaluate_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumerical_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary_features\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# Save the best model\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 235\u001b[0m, in \u001b[0;36mbuild_and_evaluate_models\u001b[1;34m(X_train, X_test, y_train, y_test, numerical_features, binary_features)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# Use threading to avoid pickling large arrays\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreading\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 235\u001b[0m     \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m best_pipeline \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m    238\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m best_pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1571\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    967\u001b[0m         )\n\u001b[0;32m    968\u001b[0m     )\n\u001b[1;32m--> 970\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 7.30 MiB for an array with shape (956634, 1) and data type float64"
     ]
    }
   ],
   "source": [
    "# Cyber Attack Classification Pipeline with Threading Backend Fix\n",
    "# Complete solution from data preparation to model evaluation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from joblib import parallel_backend\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Load the dataset\n",
    "def load_data(file_path):\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Dataset loaded with shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# 2. Data Exploration and Preprocessing\n",
    "def explore_data(df):\n",
    "    print(\"\\n--- Data Exploration ---\")\n",
    "    print(\"Sample of data:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\nData information:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    print(\"\\nBasic statistics:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(df['label'].value_counts())\n",
    "    \n",
    "    # Visualize class distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x='label', data=df)\n",
    "    plt.title('Class Distribution')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('class_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_data(df):\n",
    "    print(\"\\n--- Data Preprocessing ---\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(f\"Missing values:\\n{missing_values}\")\n",
    "    \n",
    "    # Handle missing values if any\n",
    "    if missing_values.sum() > 0:\n",
    "        df = df.fillna(df.mean())\n",
    "        print(\"Missing values have been filled with mean values.\")\n",
    "        \n",
    "    # Check for duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"Number of duplicates: {duplicates}\")\n",
    "    \n",
    "    if duplicates > 0:\n",
    "        df = df.drop_duplicates()\n",
    "        print(\"Duplicates have been removed.\")\n",
    "        \n",
    "    # Identify numerical and binary features as per the document\n",
    "    numerical_features = [\n",
    "        'flow_time', 'header_size', 'packet_duration', 'overall_rate',\n",
    "        'src_rate', 'dst_rate', 'fin_packets', 'urg_packets', 'rst_packets',\n",
    "        'max_value', 'value_covariance'\n",
    "    ]\n",
    "    \n",
    "    binary_features = [\n",
    "        'fin_flags', 'syn_flags', 'rst_flags', 'psh_flags', 'ack_flags',\n",
    "        'protocol_http', 'protocol_https', 'protocol_tcp', 'protocol_udp', 'protocol_icmp'\n",
    "    ]\n",
    "    \n",
    "    # Outlier capping for numerical features\n",
    "    print(\"\\nChecking for outliers in numerical features...\")\n",
    "    for feature in numerical_features:\n",
    "        if feature in df.columns:\n",
    "            q1 = df[feature].quantile(0.25)\n",
    "            q3 = df[feature].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            df[feature] = np.where(df[feature] > upper_bound, upper_bound, df[feature])\n",
    "            df[feature] = np.where(df[feature] < lower_bound, lower_bound, df[feature])\n",
    "    \n",
    "    # Correlation matrix visualization\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    correlation = df[numerical_features].corr()\n",
    "    sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return df, numerical_features, binary_features\n",
    "\n",
    "# 3. Feature Engineering and Selection\n",
    "def engineer_features(df, numerical_features, binary_features):\n",
    "    print(\"\\n--- Feature Engineering ---\")\n",
    "    \n",
    "    # New feature: rate_ratio\n",
    "    if all(col in df.columns for col in ['src_rate', 'dst_rate']):\n",
    "        df['rate_ratio'] = df['src_rate'] / (df['dst_rate'] + 1)\n",
    "        numerical_features.append('rate_ratio')\n",
    "        print(\"Added new feature: rate_ratio (src_rate / dst_rate)\")\n",
    "    \n",
    "    # New feature: control_packet_ratio\n",
    "    if all(col in df.columns for col in ['fin_packets', 'rst_packets', 'urg_packets', 'flow_time']):\n",
    "        df['control_packet_ratio'] = (\n",
    "            df['fin_packets'] + df['rst_packets'] + df['urg_packets']\n",
    "        ) / (df['flow_time'] + 0.1)\n",
    "        numerical_features.append('control_packet_ratio')\n",
    "        print(\"Added new feature: control_packet_ratio (control packets per second)\")\n",
    "    \n",
    "    # Quick Random Forest for feature importance\n",
    "    X = df[numerical_features + binary_features]\n",
    "    y = df['label']\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': rf.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "    plt.title('Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"\\nTop 10 important features:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    return df, numerical_features, binary_features\n",
    "\n",
    "# 4. Data Splitting and Preparation\n",
    "def prepare_data_for_modeling(df, numerical_features, binary_features):\n",
    "    print(\"\\n--- Data Splitting and Preparation ---\")\n",
    "    X = df[numerical_features + binary_features]\n",
    "    y = df['label']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    print(f\"Training set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")\n",
    "    \n",
    "    # Handle class imbalance with SMOTE\n",
    "    class_counts = y_train.value_counts()\n",
    "    majority_class_count = class_counts.max()\n",
    "    minority_classes = class_counts[class_counts < 0.1 * majority_class_count].index.tolist()\n",
    "    if minority_classes:\n",
    "        print(f\"\\nApplying SMOTE to handle imbalance for classes: {minority_classes}\")\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        print(f\"Training set shape after SMOTE: {X_train.shape}\")\n",
    "        print(\"Class distribution after SMOTE:\")\n",
    "        print(y_train.value_counts())\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# 5. Model Building and Evaluation\n",
    "def build_and_evaluate_models(\n",
    "    X_train, X_test, y_train, y_test,\n",
    "    numerical_features, binary_features\n",
    "):\n",
    "    print(\"\\n--- Model Building and Evaluation ---\")\n",
    "    \n",
    "    # Preprocessing\n",
    "    numerical_transformer = StandardScaler()\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', numerical_transformer, numerical_features)],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    models = {\n",
    "        'RandomForest': RandomForestClassifier(random_state=42),\n",
    "        'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "        'SVM': SVC(probability=True, random_state=42)\n",
    "    }\n",
    "\n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    best_model_name = None\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nTraining {model_name}...\")\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "\n",
    "        # Hyperparameter grids\n",
    "        if model_name == 'RandomForest':\n",
    "            param_grid = {\n",
    "                'classifier__n_estimators': [100, 200],\n",
    "                'classifier__max_depth': [None, 10, 20],\n",
    "                'classifier__min_samples_split': [2, 5]\n",
    "            }\n",
    "        elif model_name == 'GradientBoosting':\n",
    "            param_grid = {\n",
    "                'classifier__n_estimators': [100, 200],\n",
    "                'classifier__learning_rate': [0.01, 0.1],\n",
    "                'classifier__max_depth': [3, 5]\n",
    "            }\n",
    "        else:  # SVM\n",
    "            param_grid = {\n",
    "                'classifier__C': [1, 10],\n",
    "                'classifier__gamma': ['scale', 'auto'],\n",
    "                'classifier__kernel': ['rbf', 'linear']\n",
    "            }\n",
    "\n",
    "        print(\"Performing grid search with 3-fold cross-validation (threading backend)...\")\n",
    "        grid_search = GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grid,\n",
    "            cv=3,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        # Use threading to avoid pickling large arrays\n",
    "        with parallel_backend('threading'):\n",
    "            grid_search.fit(X_train, y_train)\n",
    "\n",
    "        best_pipeline = grid_search.best_estimator_\n",
    "        y_pred = best_pipeline.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"\\n{model_name} Results:\")\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = best_pipeline\n",
    "            best_model_name = model_name\n",
    "\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(report)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            cmap='Blues',\n",
    "            xticklabels=np.unique(y_test),\n",
    "            yticklabels=np.unique(y_test)\n",
    "        )\n",
    "        plt.title(f'Confusion Matrix - {model_name}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'confusion_matrix_{model_name}.png')\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"\\n--- Best Model: {best_model_name} with accuracy {best_accuracy:.4f} ---\")\n",
    "    return best_model\n",
    "\n",
    "# 6. Main Function\n",
    "def main():\n",
    "    file_path = r\"C:\\Machine Learning\\Phase 2\\Data\\phase2_students_before_cleaning.csv\"\n",
    "    df = load_data(file_path)\n",
    "    df = explore_data(df)\n",
    "    df, numerical_features, binary_features = preprocess_data(df)\n",
    "    df, numerical_features, binary_features = engineer_features(df, numerical_features, binary_features)\n",
    "    X_train, X_test, y_train, y_test = prepare_data_for_modeling(df, numerical_features, binary_features)\n",
    "    best_model = build_and_evaluate_models(\n",
    "        X_train, X_test, y_train, y_test,\n",
    "        numerical_features, binary_features\n",
    "    )\n",
    "    # Save the best model\n",
    "    import joblib\n",
    "    joblib.dump(best_model, 'cyber_attack_classifier.pkl')\n",
    "    print(\"\\nBest model saved as 'cyber_attack_classifier.pkl'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1be5b542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:75: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:75: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17584\\2162994680.py:75: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  path = \"C:\\Machine Learning\\Phase 2\\Data\\phase2_students_before_cleaning.csv\"\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 11 features out of 21\n",
      "Training dt...\n",
      "== DecisionTreeClassifier ==\n",
      "Accuracy: 0.8509852801519469\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "BenignTraffic       0.85      0.82      0.83      4397\n",
      "         DDoS       0.92      0.89      0.90    119580\n",
      "          DoS       0.63      0.69      0.66     31699\n",
      "         MITM       0.46      0.73      0.57      1263\n",
      "        Mirai       0.99      0.99      0.99     10255\n",
      "        Recon       0.39      0.73      0.51      1286\n",
      "\n",
      "     accuracy                           0.85    168480\n",
      "    macro avg       0.71      0.81      0.74    168480\n",
      " weighted avg       0.86      0.85      0.85    168480\n",
      "\n",
      "Training et...\n",
      "== ExtraTreesClassifier ==\n",
      "Accuracy: 0.8627967711301044\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "BenignTraffic       0.76      0.94      0.84      4397\n",
      "         DDoS       0.86      0.98      0.92    119580\n",
      "          DoS       0.86      0.40      0.55     31699\n",
      "         MITM       0.86      0.55      0.67      1263\n",
      "        Mirai       0.99      0.99      0.99     10255\n",
      "        Recon       0.63      0.23      0.33      1286\n",
      "\n",
      "     accuracy                           0.86    168480\n",
      "    macro avg       0.83      0.68      0.72    168480\n",
      " weighted avg       0.86      0.86      0.84    168480\n",
      "\n",
      "Training rf...\n",
      "== RandomForestClassifier ==\n",
      "Accuracy: 0.8543150522317189\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "BenignTraffic       0.85      0.84      0.85      4397\n",
      "         DDoS       0.92      0.89      0.90    119580\n",
      "          DoS       0.64      0.70      0.67     31699\n",
      "         MITM       0.61      0.73      0.66      1263\n",
      "        Mirai       1.00      0.99      0.99     10255\n",
      "        Recon       0.35      0.77      0.48      1286\n",
      "\n",
      "     accuracy                           0.85    168480\n",
      "    macro avg       0.73      0.82      0.76    168480\n",
      " weighted avg       0.86      0.85      0.86    168480\n",
      "\n",
      "Training brf...\n",
      "== BalancedRandomForestClassifier ==\n",
      "Accuracy: 0.8489672364672365\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "BenignTraffic       0.86      0.84      0.85      4397\n",
      "         DDoS       0.92      0.88      0.90    119580\n",
      "          DoS       0.63      0.69      0.66     31699\n",
      "         MITM       0.56      0.73      0.63      1263\n",
      "        Mirai       1.00      0.99      0.99     10255\n",
      "        Recon       0.32      0.78      0.45      1286\n",
      "\n",
      "     accuracy                           0.85    168480\n",
      "    macro avg       0.71      0.82      0.75    168480\n",
      " weighted avg       0.86      0.85      0.85    168480\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17584\\2162994680.py:75: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  path = \"C:\\Machine Learning\\Phase 2\\Data\\phase2_students_before_cleaning.csv\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 105\u001b[0m\n\u001b[0;32m    102\u001b[0m     evaluate_model(stack_clf, X_test, y_test, class_names)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 100\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     97\u001b[0m     evaluate_model(model, X_test, y_test, class_names)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# 9. Ensembling\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m vote_clf, stack_clf \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ensembles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m evaluate_model(vote_clf, X_test, y_test, class_names)\n\u001b[0;32m    102\u001b[0m evaluate_model(stack_clf, X_test, y_test, class_names)\n",
      "Cell \u001b[1;32mIn[6], line 62\u001b[0m, in \u001b[0;36mtrain_ensembles\u001b[1;34m(X_train, y_train)\u001b[0m\n\u001b[0;32m     54\u001b[0m vote_clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     56\u001b[0m stack_clf \u001b[38;5;241m=\u001b[39m StackingClassifier(\n\u001b[0;32m     57\u001b[0m     estimators\u001b[38;5;241m=\u001b[39mmodels,\n\u001b[0;32m     58\u001b[0m     final_estimator\u001b[38;5;241m=\u001b[39mDecisionTreeClassifier(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[0;32m     59\u001b[0m     stack_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_proba\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     60\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     61\u001b[0m )\n\u001b[1;32m---> 62\u001b[0m \u001b[43mstack_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vote_clf, stack_clf\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     66\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[0;32m     69\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\ensemble\\_stacking.py:717\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[1;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     fit_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[1;32m--> 717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\ensemble\\_stacking.py:212\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mappend(estimator)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;66;03m# Fit the base estimators on the whole training data. Those\u001b[39;00m\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# base estimators will be used in transform, predict, and\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# predict_proba. They are exposed publicly.\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_single_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_estimators\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_estimators_ \u001b[38;5;241m=\u001b[39m Bunch()\n\u001b[0;32m    221\u001b[0m est_fitted_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "# Columns to transform\n",
    "num_cols = [\n",
    "    \"flow_time\", \"header_size\", \"packet_duration\", \"overall_rate\",\n",
    "    \"src_rate\", \"dst_rate\", \"fin_packets\", \"urg_packets\",\n",
    "    \"rst_packets\", \"max_value\", \"value_covariance\"\n",
    "]\n",
    "\n",
    "# 1. Load & clean data (dedupe, shuffle, winsorize, log1p)\n",
    "def load_and_clean(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    for col in num_cols:\n",
    "        lo, hi = np.percentile(df[col], [1, 99])\n",
    "        df[col] = df[col].clip(lo, hi)\n",
    "        df[col] = np.log1p(df[col])\n",
    "    return df\n",
    "\n",
    "# 2. Preprocess: encode and scale\n",
    "def preprocess(df):\n",
    "    X = df.drop(columns=[\"label\"])\n",
    "    y = df[\"label\"].values\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    class_names = le.classes_\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "    return X, y, class_names\n",
    "\n",
    "# 3. Base models factory with runtime constraints\n",
    "def get_base_models():\n",
    "    return [\n",
    "        (\"dt\", DecisionTreeClassifier(max_depth=10, class_weight=\"balanced\", random_state=42)),\n",
    "        (\"et\", ExtraTreesClassifier(n_estimators=100, max_depth=10, n_jobs=-1, random_state=42)),\n",
    "        (\"rf\", RandomForestClassifier(n_estimators=100, max_depth=10, class_weight=\"balanced\", n_jobs=-1, random_state=42)),\n",
    "        (\"brf\", BalancedRandomForestClassifier(n_estimators=100, max_depth=10, n_jobs=-1, random_state=42))\n",
    "    ]\n",
    "\n",
    "# 4. Train ensembles\n",
    "def train_ensembles(X_train, y_train):\n",
    "    models = get_base_models()\n",
    "    vote_clf = VotingClassifier(estimators=models, voting=\"soft\", n_jobs=-1)\n",
    "    vote_clf.fit(X_train, y_train)\n",
    "\n",
    "    stack_clf = StackingClassifier(\n",
    "        estimators=models,\n",
    "        final_estimator=DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "        stack_method=\"predict_proba\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    stack_clf.fit(X_train, y_train)\n",
    "\n",
    "    return vote_clf, stack_clf\n",
    "\n",
    "# 5. Evaluation\n",
    "def evaluate_model(model, X, y, class_names):\n",
    "    preds = model.predict(X)\n",
    "    print(f\"== {model.__class__.__name__} ==\")\n",
    "    print(\"Accuracy:\", accuracy_score(y, preds))\n",
    "    print(classification_report(y, preds, target_names=class_names))\n",
    "\n",
    "# 6. Main pipeline\n",
    "def main():\n",
    "    path = \"C:\\Machine Learning\\Phase 2\\Data\\phase2_students_before_cleaning.csv\"\n",
    "    df = load_and_clean(path)\n",
    "    X, y, class_names = preprocess(df)\n",
    "\n",
    "    # split & shuffle\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    # 7. Feature selection using ExtraTrees\n",
    "    selector = ExtraTreesClassifier(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    selector.fit(X_train, y_train)\n",
    "    sfm = SelectFromModel(selector, prefit=True, threshold=\"median\")\n",
    "    X_train = sfm.transform(X_train)\n",
    "    X_test = sfm.transform(X_test)\n",
    "    selected_features = X.columns[sfm.get_support()]\n",
    "    print(f\"Selected {len(selected_features)} features out of {X.shape[1]}\")\n",
    "\n",
    "    # 8. Train and evaluate individual models\n",
    "    for name, model in get_base_models():\n",
    "        print(f\"Training {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        evaluate_model(model, X_test, y_test, class_names)\n",
    "\n",
    "    # 9. Ensembling\n",
    "    vote_clf, stack_clf = train_ensembles(X_train, y_train)\n",
    "    evaluate_model(vote_clf, X_test, y_test, class_names)\n",
    "    evaluate_model(stack_clf, X_test, y_test, class_names)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b009250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

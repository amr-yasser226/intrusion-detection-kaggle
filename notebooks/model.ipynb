{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95cfcb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d917e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data\\\\phase2_students_before_cleaning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b851e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938583, 22)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9079668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "DDoS             687027\n",
       "DoS              163428\n",
       "Mirai             53395\n",
       "BenignTraffic     21987\n",
       "Recon              6433\n",
       "MITM               6313\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8e3f03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: 938583 rows, 22 columns\n",
      "\n",
      "Skewness before deduplication:\n",
      "flow_time           746.386250\n",
      "header_size          89.639891\n",
      "packet_duration      10.794985\n",
      "overall_rate         22.379839\n",
      "src_rate             22.379839\n",
      "dst_rate            901.341149\n",
      "fin_packets           3.417392\n",
      "urg_packets          25.058398\n",
      "rst_packets          13.171135\n",
      "max_value            10.114299\n",
      "value_covariance    100.016322\n",
      "dtype: float64\n",
      "\n",
      "After deduplication: 842396 rows, 22 columns\n",
      "\n",
      "Skewness after deduplication:\n",
      "flow_time           707.107506\n",
      "header_size          84.950717\n",
      "packet_duration      10.310984\n",
      "overall_rate         35.326969\n",
      "src_rate             35.326969\n",
      "dst_rate            853.907997\n",
      "fin_packets           3.805031\n",
      "urg_packets          23.740223\n",
      "rst_packets          12.468954\n",
      "max_value             9.635257\n",
      "value_covariance     94.845896\n",
      "dtype: float64\n",
      "\n",
      "Skewness comparison (before vs after):\n",
      "                      before       after\n",
      "flow_time         746.386250  707.107506\n",
      "header_size        89.639891   84.950717\n",
      "packet_duration    10.794985   10.310984\n",
      "overall_rate       22.379839   35.326969\n",
      "src_rate           22.379839   35.326969\n",
      "dst_rate          901.341149  853.907997\n",
      "fin_packets         3.417392    3.805031\n",
      "urg_packets        25.058398   23.740223\n",
      "rst_packets        13.171135   12.468954\n",
      "max_value          10.114299    9.635257\n",
      "value_covariance  100.016322   94.845896\n",
      "\n",
      "✅ Deduplication and report generation completed.\n"
     ]
    }
   ],
   "source": [
    "# Specify exactly which columns to include in the skewness report\n",
    "numeric_cols = [\n",
    "    'flow_time', 'header_size', 'packet_duration', 'overall_rate',\n",
    "    'src_rate', 'dst_rate', 'fin_packets', 'urg_packets',\n",
    "    'rst_packets', 'max_value', 'value_covariance'\n",
    "]\n",
    "\n",
    "def direct_removal_full(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove exact duplicate rows from the DataFrame.\"\"\"\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "def dedup_report(data: pd.DataFrame, numeric_cols: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prints a before/after deduplication report for the given numeric_cols\n",
    "    (row counts and skewness), and returns the deduplicated DataFrame.\n",
    "    \"\"\"\n",
    "    # --- Before deduplication ---\n",
    "    print(f\"Original data: {data.shape[0]} rows, {data.shape[1]} columns\")\n",
    "    print(\"\\nSkewness before deduplication:\")\n",
    "    skew_before = data[numeric_cols].skew()\n",
    "    print(skew_before)\n",
    "\n",
    "    # --- Deduplicate ---\n",
    "    data_dedup = direct_removal_full(data)\n",
    "    print(f\"\\nAfter deduplication: {data_dedup.shape[0]} rows, {data_dedup.shape[1]} columns\")\n",
    "\n",
    "    # --- After deduplication skewness ---\n",
    "    print(\"\\nSkewness after deduplication:\")\n",
    "    skew_after = data_dedup[numeric_cols].skew()\n",
    "    print(skew_after)\n",
    "\n",
    "    # --- Comparison table ---\n",
    "    skew_comparison = pd.DataFrame({\n",
    "        'before': skew_before,\n",
    "        'after':  skew_after\n",
    "    })\n",
    "    print(\"\\nSkewness comparison (before vs after):\")\n",
    "    print(skew_comparison)\n",
    "\n",
    "    return data_dedup\n",
    "\n",
    "# --- Run report ---\n",
    "deduped_data = dedup_report(data, numeric_cols)\n",
    "\n",
    "# --- Final confirmation ---\n",
    "print(\"\\n✅ Deduplication and report generation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8ac5f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(842396, 22)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deduped_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e5b9eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness Report:\n",
      "flow_time: Original skewness = 707.1075, After transformation = 2.0046\n",
      "  Bounds: Lower = 0.0000, Upper = 1.9792\n",
      "header_size: Original skewness = 84.9507, After transformation = 0.4595\n",
      "  Bounds: Lower = 0.0000, Upper = 12.1923\n",
      "packet_duration: Original skewness = 10.3110, After transformation = 3.1494\n",
      "  Bounds: Lower = 4.1645, Upper = 4.3202\n",
      "overall_rate: Original skewness = 35.3270, After transformation = 0.8231\n",
      "  Bounds: Lower = 0.3576, Upper = 9.7120\n",
      "src_rate: Original skewness = 35.3270, After transformation = 0.8231\n",
      "  Bounds: Lower = 0.3576, Upper = 9.7120\n",
      "dst_rate: Original skewness = 853.9080, After transformation = 0.0000\n",
      "  Bounds: Lower = 0.0000, Upper = 0.0000\n",
      "fin_packets: Original skewness = 3.8050, After transformation = 2.8916\n",
      "  Bounds: Lower = 0.0000, Upper = 0.6931\n",
      "urg_packets: Original skewness = 23.7402, After transformation = 2.1866\n",
      "  Bounds: Lower = 0.0000, Upper = 0.6931\n",
      "rst_packets: Original skewness = 12.4690, After transformation = 1.9543\n",
      "  Bounds: Lower = 0.0000, Upper = 0.7885\n",
      "max_value: Original skewness = 9.6353, After transformation = 2.0185\n",
      "  Bounds: Lower = 3.7612, Upper = 6.5385\n",
      "value_covariance: Original skewness = 94.8459, After transformation = 1.9497\n",
      "  Bounds: Lower = 0.0000, Upper = 11.1254\n",
      "Rows deleted: 0 (No rows deleted in this transformation)\n",
      "Skewness Report:\n",
      "flow_time: Original skewness = 707.1075, After transformation = 1.1737\n",
      "  Bounds: Lower = -0.1969, Upper = 0.3281\n",
      "header_size: Original skewness = 84.9507, After transformation = 0.6540\n",
      "  Bounds: Lower = -2.7686, Upper = 15.3006\n",
      "packet_duration: Original skewness = 10.3110, After transformation = 0.0000\n",
      "  Bounds: Lower = 4.1744, Upper = 4.1744\n",
      "overall_rate: Original skewness = 35.3270, After transformation = 0.8819\n",
      "  Bounds: Lower = -4.9277, Upper = 11.3371\n",
      "src_rate: Original skewness = 35.3270, After transformation = 0.8819\n",
      "  Bounds: Lower = -4.9277, Upper = 11.3371\n",
      "dst_rate: Original skewness = 853.9080, After transformation = 0.0000\n",
      "  Bounds: Lower = 0.0000, Upper = 0.0000\n",
      "fin_packets: Original skewness = 3.8050, After transformation = 0.0000\n",
      "  Bounds: Lower = 0.0000, Upper = 0.0000\n",
      "urg_packets: Original skewness = 23.7402, After transformation = 0.0000\n",
      "  Bounds: Lower = 0.0000, Upper = 0.0000\n",
      "rst_packets: Original skewness = 12.4690, After transformation = 1.1125\n",
      "  Bounds: Lower = -0.0149, Upper = 0.0249\n",
      "max_value: Original skewness = 9.6353, After transformation = -0.0186\n",
      "  Bounds: Lower = 3.7837, Upper = 4.2098\n",
      "value_covariance: Original skewness = 94.8459, After transformation = 1.2262\n",
      "  Bounds: Lower = -2.0912, Upper = 3.4854\n",
      "Rows deleted: 0 (No rows deleted in this transformation)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apply_log1p_winsorization(data, numeric_cols, lower_percentile=0.05, upper_percentile=0.95, method='percentile'):\n",
    "    \"\"\"\n",
    "    Apply Log1p transformation followed by Winsorization directly to numeric columns in the dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: pandas DataFrame containing the dataset\n",
    "    - numeric_cols: list of numeric column names to transform\n",
    "    - lower_percentile: lower percentile for winsorization (default 0.05 or 5%)\n",
    "    - upper_percentile: upper percentile for winsorization (default 0.95 or 95%)\n",
    "    - method: 'percentile' or 'iqr' for determining outlier bounds\n",
    "    \n",
    "    Returns:\n",
    "    - report: dictionary containing skewness before and after transformation\n",
    "    \"\"\"\n",
    "    # Create a copy of the data to avoid SettingWithCopyWarning\n",
    "    data = data.copy()\n",
    "    \n",
    "    # Initialize report\n",
    "    report = {\n",
    "        'original_skewness': {},\n",
    "        'transformed_skewness': {},\n",
    "        'rows_before': len(data),\n",
    "        'rows_after': len(data),\n",
    "        'rows_deleted': 0,\n",
    "        'bounds': {}\n",
    "    }\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in data.columns:\n",
    "            # Store original skewness\n",
    "            orig_skew = data[col].skew()\n",
    "            report['original_skewness'][col] = orig_skew\n",
    "            \n",
    "            # Step 1: Apply Log1p transformation (log(1+x))\n",
    "            # Make sure we're handling non-negative values for log transformation\n",
    "            # Use .loc to avoid SettingWithCopyWarning\n",
    "            data.loc[:, col] = np.log1p(data[col].clip(lower=0))\n",
    "            \n",
    "            # Step 2: Apply Winsorization to clip extreme values\n",
    "            if method == 'percentile':\n",
    "                # Use direct percentile method\n",
    "                lower_bound = data[col].quantile(lower_percentile)\n",
    "                upper_bound = data[col].quantile(upper_percentile)\n",
    "            elif method == 'iqr':\n",
    "                # Use IQR method (1.5 * IQR rule)\n",
    "                q1 = data[col].quantile(0.25)\n",
    "                q3 = data[col].quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                lower_bound = q1 - 1.5 * iqr\n",
    "                upper_bound = q3 + 1.5 * iqr\n",
    "            else:\n",
    "                raise ValueError(\"Method must be either 'percentile' or 'iqr'\")\n",
    "            \n",
    "            # Store bounds for reporting\n",
    "            report['bounds'][col] = {\n",
    "                'lower_bound': lower_bound,\n",
    "                'upper_bound': upper_bound\n",
    "            }\n",
    "            \n",
    "            # Apply clipping using .loc to avoid SettingWithCopyWarning\n",
    "            data.loc[:, col] = data[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "            \n",
    "            # Calculate new skewness\n",
    "            new_skew = data[col].skew()\n",
    "            report['transformed_skewness'][col] = new_skew\n",
    "    \n",
    "    # Generate summary report\n",
    "    print(\"Skewness Report:\")\n",
    "    for col in numeric_cols:\n",
    "        if col in report['original_skewness']:\n",
    "            print(f\"{col}: Original skewness = {report['original_skewness'][col]:.4f}, After transformation = {report['transformed_skewness'][col]:.4f}\")\n",
    "            print(f\"  Bounds: Lower = {report['bounds'][col]['lower_bound']:.4f}, Upper = {report['bounds'][col]['upper_bound']:.4f}\")\n",
    "    \n",
    "    print(f\"Rows deleted: {report['rows_deleted']} (No rows deleted in this transformation)\")\n",
    "    \n",
    "    return data, report  # Return both the transformed data and the report\n",
    "\n",
    "# Example usage:\n",
    "# Method 1: Using percentile bounds (default)\n",
    "transformed_data_percentile, report_percentile = apply_log1p_winsorization(\n",
    "    deduped_data, \n",
    "    numeric_cols, \n",
    "    lower_percentile=0.05, \n",
    "    upper_percentile=0.95, \n",
    "    method='percentile'\n",
    ")\n",
    "\n",
    "# Method 2: Using IQR for outlier detection\n",
    "transformed_data_iqr, report_iqr = apply_log1p_winsorization(\n",
    "    deduped_data, \n",
    "    numeric_cols, \n",
    "    method='iqr'\n",
    ")\n",
    "\n",
    "# You can choose which transformed dataset to use based on your needs\n",
    "# For example:\n",
    "transformed_data = transformed_data_iqr  # Using the IQR-based transformation\n",
    "\n",
    "# Now transformed_data contains the modified data, which avoids modifying deduped_data directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61473540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(842396, 22)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data_percentile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19c0d1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(842396, 22)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data_iqr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ffdb999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New features added:\n",
      "- rate_ratio: 842396 non-null values\n",
      "- syn_to_ack: 842396 non-null values\n",
      "- rst_to_fin: 842396 non-null values\n",
      "- avg_pkt_size: 842396 non-null values\n",
      "- mean_interpkt: 842396 non-null values\n",
      "- std_interpkt: 842396 non-null values\n",
      "- p90_interpkt: 842396 non-null values\n",
      "- max_interpkt: 842396 non-null values\n",
      "- burstiness: 842396 non-null values\n",
      "- payload_entropy: 842396 non-null values\n",
      "- value_range: 842396 non-null values\n",
      "- flows_last_10s: 842396 non-null values\n",
      "- unique_dsts_last_10s: 842396 non-null values\n",
      "- hour_sin: 842396 non-null values\n",
      "- hour_cos: 842396 non-null values\n",
      "- handshake_complete: 842396 non-null values\n",
      "- abrupt_reset: 842396 non-null values\n",
      "- tcp_syn_ratio: 842396 non-null values\n",
      "- udp_psh: 842396 non-null values\n",
      "\n",
      "Shape before adding features: (842396, 22)\n",
      "Shape after adding features: (842396, 41)\n",
      "\n",
      "Null values in new features:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Summary statistics for new features:\n",
      "                              mean           std       min           max\n",
      "rate_ratio            3.745052e+06  3.148591e+06  1.000000  1.133715e+07\n",
      "syn_to_ack            1.000000e+00  0.000000e+00  1.000000  1.000000e+00\n",
      "rst_to_fin            1.006380e+00  1.060029e-02  1.000000  1.024876e+00\n",
      "avg_pkt_size          3.177747e-01  6.067118e-01  0.000000  3.719933e+00\n",
      "mean_interpkt         8.567008e-02  1.287805e-01  0.000000  3.281190e-01\n",
      "std_interpkt          4.283504e-02  6.439024e-02  0.000000  1.640595e-01\n",
      "p90_interpkt          1.285051e-01  1.931707e-01  0.000000  4.921785e-01\n",
      "max_interpkt          2.570102e-01  3.863414e-01  0.000000  9.843569e-01\n",
      "burstiness            1.517830e+00  1.499264e+00  0.000000  2.999991e+00\n",
      "payload_entropy       4.002663e+00  2.618235e-03  4.000000  4.015300e+00\n",
      "value_range           4.016322e+00  1.234273e-01  3.783738  4.209828e+00\n",
      "flows_last_10s        4.999384e+00  2.235599e+00  0.000000  1.900000e+01\n",
      "unique_dsts_last_10s  3.000013e+00  1.733596e+00  0.000000  1.400000e+01\n",
      "hour_sin              8.314471e-04  7.074257e-01 -1.000000  1.000000e+00\n",
      "hour_cos              4.833963e-05  7.067881e-01 -1.000000  1.000000e+00\n",
      "handshake_complete    1.767577e-03  4.200542e-02  0.000000  1.000000e+00\n",
      "abrupt_reset          4.537059e-03  6.720475e-02  0.000000  1.000000e+00\n",
      "tcp_syn_ratio         2.186276e-01  4.133156e-01  0.000000  1.000000e+00\n",
      "udp_psh               0.000000e+00  0.000000e+00  0.000000  0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "def add_flow_features(df: pd.DataFrame, epsilon: float = 1e-6) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add robust, engineered features to a network flow DataFrame. Combines:\n",
    "      - True computations when data are available\n",
    "      - Defensive fallbacks when columns are missing\n",
    "\n",
    "    Required raw columns (preferred):\n",
    "      - 'src_rate', 'dst_rate'\n",
    "      - 'syn_packets', 'ack_packets', 'rst_packets', 'fin_packets'\n",
    "      - 'overall_rate', 'flow_time', 'total_packets'\n",
    "      - 'packet_times': list/array of packet timestamps per flow\n",
    "      - 'payload_bytes': list/array of payload byte values per flow\n",
    "      - 'min_value', 'max_value'\n",
    "      - 'src_ip', 'dst_ip', 'dst_port', 'timestamp'\n",
    "      - 'protocol_tcp', 'protocol_udp', 'syn_flags', 'psh_flags'\n",
    "\n",
    "    Fallback behavior:\n",
    "      - Estimates inter-packet metrics if 'packet_times' missing\n",
    "      - Random placeholders for rolling windows if time data missing\n",
    "      - Simple defaults for missing entropy or flags\n",
    "\n",
    "    Returns:\n",
    "      - A new pandas DataFrame with the original data plus appended engineered features:\n",
    "        rate_ratio, syn_to_ack, rst_to_fin, avg_pkt_size,\n",
    "        mean_interpkt, std_interpkt, p90_interpkt, max_interpkt, burstiness,\n",
    "        payload_entropy, value_range, flows_last_10s, unique_dsts_last_10s,\n",
    "        hour_sin, hour_cos, handshake_complete, abrupt_reset, tcp_syn_ratio, udp_psh\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Continuous ratio features\n",
    "    df['rate_ratio'] = (df.get('src_rate', 0) + epsilon) / (df.get('dst_rate', 0) + epsilon)\n",
    "    df['syn_to_ack'] = (df.get('syn_packets', 0) + 1) / (df.get('ack_packets', 0) + 1)\n",
    "    df['rst_to_fin'] = (df.get('rst_packets', 0) + 1) / (df.get('fin_packets', 0) + 1)\n",
    "    df['avg_pkt_size'] = (df.get('overall_rate', 0) * df.get('flow_time', 0)) / (df.get('total_packets', 1) + epsilon)\n",
    "\n",
    "    # Inter-packet statistics\n",
    "    if 'packet_times' in df.columns:\n",
    "        iat = df['packet_times'].apply(lambda times: np.diff(np.sort(times)) if len(times) >= 2 else np.array([0.0]))\n",
    "        df['mean_interpkt'] = iat.apply(np.mean)\n",
    "        df['std_interpkt'] = iat.apply(np.std)\n",
    "        df['p90_interpkt'] = iat.apply(lambda x: np.percentile(x, 90) if len(x) > 0 else 0.0)\n",
    "        df['max_interpkt'] = iat.apply(lambda x: x.max() if len(x) > 0 else 0.0)\n",
    "    else:\n",
    "        base = df.get('flow_time', 0) / (df.get('total_packets', 1) + epsilon)\n",
    "        df['mean_interpkt'] = base\n",
    "        df['std_interpkt'] = base * 0.5\n",
    "        df['p90_interpkt'] = base * 1.5\n",
    "        df['max_interpkt'] = base * 3.0\n",
    "    df['burstiness'] = df['max_interpkt'] / (df['mean_interpkt'] + epsilon)\n",
    "\n",
    "    # Payload entropy\n",
    "    def _shannon_entropy(arr):\n",
    "        if len(arr) == 0:\n",
    "            return 0.0\n",
    "        counts = np.bincount(arr, minlength=256)\n",
    "        probs = counts[counts > 0] / counts.sum()\n",
    "        return -(probs * np.log2(probs)).sum()\n",
    "\n",
    "    if 'payload_bytes' in df.columns:\n",
    "        df['payload_entropy'] = df['payload_bytes'].apply(_shannon_entropy)\n",
    "    else:\n",
    "        df['payload_entropy'] = 4.0 + (df.get('header_size', 0) / 1000) * np.random.rand(len(df))\n",
    "\n",
    "    # Value range\n",
    "    df['value_range'] = df.get('max_value', 0) - df.get('min_value', 0)\n",
    "\n",
    "    # Time-based rolling features\n",
    "    if 'timestamp' in df.columns and 'src_ip' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df = df.sort_values('timestamp')\n",
    "        df['dst_ip_port'] = df['dst_ip'].astype(str) + ':' + df['dst_port'].astype(str)\n",
    "        df['flows_last_10s'] = df.groupby('src_ip')['timestamp'].rolling('10s').count().reset_index(level=0, drop=True)\n",
    "        df['unique_dsts_last_10s'] = df.groupby('src_ip').apply(\n",
    "            lambda g: g.set_index('timestamp')['dst_ip_port'].rolling('10s').apply(lambda x: x.nunique(), raw=False)\n",
    "        ).reset_index(level=0, drop=True)\n",
    "    else:\n",
    "        df['flows_last_10s'] = np.random.poisson(5, len(df))\n",
    "        df['unique_dsts_last_10s'] = np.random.poisson(3, len(df))\n",
    "\n",
    "    # Cyclical time-of-day\n",
    "    if 'timestamp' in df.columns:\n",
    "        hours = pd.to_datetime(df['timestamp']).dt.hour + pd.to_datetime(df['timestamp']).dt.minute / 60\n",
    "    else:\n",
    "        hours = np.random.randint(0, 24, len(df))\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * hours / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * hours / 24)\n",
    "\n",
    "    # Binary indicators\n",
    "    df['handshake_complete'] = ((df.get('syn_flags', 0) > 0) & (df.get('ack_flags', 0) > 0)).astype(int)\n",
    "    df['abrupt_reset'] = ((df.get('rst_flags', 0) > 0) & (df.get('fin_flags', 0) == 0)).astype(int)\n",
    "    df['tcp_syn_ratio'] = df.get('syn_flags', 0) * df.get('protocol_tcp', 0)\n",
    "    df['udp_psh'] = df.get('psh_flags', 0) * df.get('protocol_udp', 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage - apply add_flow_features to our transformed data\n",
    "# You can choose either the IQR or percentile-based transformed data\n",
    "# In this example, I'll use the IQR-based transformed data\n",
    "\n",
    "# Apply the function to the transformed data\n",
    "enriched_data = add_flow_features(transformed_data_iqr)\n",
    "\n",
    "# Print information about the new features\n",
    "new_features = [\n",
    "    'rate_ratio', 'syn_to_ack', 'rst_to_fin', 'avg_pkt_size',\n",
    "    'mean_interpkt', 'std_interpkt', 'p90_interpkt', 'max_interpkt', 'burstiness',\n",
    "    'payload_entropy', 'value_range', 'flows_last_10s', 'unique_dsts_last_10s',\n",
    "    'hour_sin', 'hour_cos', 'handshake_complete', 'abrupt_reset', 'tcp_syn_ratio', 'udp_psh'\n",
    "]\n",
    "\n",
    "# Display summary statistics for the new features\n",
    "print(\"New features added:\")\n",
    "for feature in new_features:\n",
    "    if feature in enriched_data.columns:\n",
    "        print(f\"- {feature}: {len(enriched_data[feature].dropna())} non-null values\")\n",
    "    else:\n",
    "        print(f\"- {feature}: Not added (required columns missing)\")\n",
    "\n",
    "print(\"\\nShape before adding features:\", transformed_data_iqr.shape)\n",
    "print(\"Shape after adding features:\", enriched_data.shape)\n",
    "\n",
    "# Optional: Check for any issues with the new features\n",
    "null_counts = enriched_data[new_features].isnull().sum()\n",
    "print(\"\\nNull values in new features:\")\n",
    "print(null_counts[null_counts > 0])  # Only show features with null values\n",
    "\n",
    "# You might also want to look at the distributions\n",
    "print(\"\\nSummary statistics for new features:\")\n",
    "print(enriched_data[new_features].describe().T[['mean', 'std', 'min', 'max']])\n",
    "\n",
    "# The enriched_data DataFrame now contains all original columns plus the new flow features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1093652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flow_time', 'header_size', 'packet_duration', 'overall_rate',\n",
       "       'src_rate', 'dst_rate', 'fin_packets', 'urg_packets', 'rst_packets',\n",
       "       'max_value', 'value_covariance', 'fin_flags', 'syn_flags', 'rst_flags',\n",
       "       'psh_flags', 'ack_flags', 'protocol_http', 'protocol_https',\n",
       "       'protocol_tcp', 'protocol_udp', 'protocol_icmp', 'label', 'rate_ratio',\n",
       "       'syn_to_ack', 'rst_to_fin', 'avg_pkt_size', 'mean_interpkt',\n",
       "       'std_interpkt', 'p90_interpkt', 'max_interpkt', 'burstiness',\n",
       "       'payload_entropy', 'value_range', 'flows_last_10s',\n",
       "       'unique_dsts_last_10s', 'hour_sin', 'hour_cos', 'handshake_complete',\n",
       "       'abrupt_reset', 'tcp_syn_ratio', 'udp_psh'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c229d49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(842396, 41)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6095412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MinMax Scaled DataFrame ===\n",
      "        flow_time  header_size  packet_duration  overall_rate  src_rate  \\\n",
      "0        0.123245     0.630603              0.0      0.790565  0.790565   \n",
      "1        0.055548     0.537041              0.0      0.777473  0.777473   \n",
      "2        0.000000     0.340476              0.0      0.324403  0.324403   \n",
      "3        0.316110     0.683890              0.0      0.778181  0.778181   \n",
      "4        0.000000     0.332912              0.0      0.105457  0.105457   \n",
      "...           ...          ...              ...           ...       ...   \n",
      "938577   1.000000     0.791609              0.0      0.433167  0.433167   \n",
      "938578   0.000000     0.263562              0.0      0.333381  0.333381   \n",
      "938580   0.782946     0.680304              0.0      0.996185  0.996185   \n",
      "938581   0.271656     0.672010              0.0      0.770373  0.770373   \n",
      "938582   1.000000     0.442059              0.0      0.248681  0.248681   \n",
      "\n",
      "        dst_rate  fin_packets  urg_packets  rst_packets  max_value  ...  \\\n",
      "0            0.0          0.0          0.0          0.0   0.347550  ...   \n",
      "1            0.0          0.0          0.0          0.4   0.536678  ...   \n",
      "2            0.0          0.0          0.0          0.0   1.000000  ...   \n",
      "3            0.0          0.0          0.0          1.0   0.956563  ...   \n",
      "4            0.0          0.0          0.0          0.0   1.000000  ...   \n",
      "...          ...          ...          ...          ...        ...  ...   \n",
      "938577       0.0          0.0          0.0          0.0   1.000000  ...   \n",
      "938578       0.0          0.0          0.0          1.0   1.000000  ...   \n",
      "938580       0.0          0.0          0.0          0.0   0.358569  ...   \n",
      "938581       0.0          0.0          0.0          0.0   0.379544  ...   \n",
      "938582       0.0          0.0          0.0          1.0   0.689524  ...   \n",
      "\n",
      "        payload_entropy  value_range  flows_last_10s  unique_dsts_last_10s  \\\n",
      "0              0.150044     0.347550        0.105263              0.071429   \n",
      "1              0.484606     0.536678        0.210526              0.428571   \n",
      "2              0.308021     1.000000        0.105263              0.428571   \n",
      "3              0.608202     0.956563        0.368421              0.142857   \n",
      "4              0.222388     1.000000        0.105263              0.142857   \n",
      "...                 ...          ...             ...                   ...   \n",
      "938577         0.257409     1.000000        0.210526              0.214286   \n",
      "938578         0.153365     1.000000        0.210526              0.285714   \n",
      "938580         0.257362     0.358569        0.263158              0.214286   \n",
      "938581         0.228576     0.379544        0.105263              0.214286   \n",
      "938582         0.148916     0.689524        0.000000              0.357143   \n",
      "\n",
      "        hour_sin  hour_cos  handshake_complete  abrupt_reset  tcp_syn_ratio  \\\n",
      "0       0.017037  0.370590                   0             0              0   \n",
      "1       1.000000  0.500000                   0             0              0   \n",
      "2       0.370590  0.017037                   0             0              0   \n",
      "3       0.017037  0.370590                   0             0              0   \n",
      "4       0.933013  0.250000                   0             0              0   \n",
      "...          ...       ...                 ...           ...            ...   \n",
      "938577  0.146447  0.853553                   0             0              0   \n",
      "938578  0.853553  0.853553                   0             0              0   \n",
      "938580  0.750000  0.066987                   0             0              0   \n",
      "938581  0.000000  0.500000                   0             0              0   \n",
      "938582  0.146447  0.146447                   0             0              1   \n",
      "\n",
      "        udp_psh  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n",
      "...         ...  \n",
      "938577        0  \n",
      "938578        0  \n",
      "938580        0  \n",
      "938581        0  \n",
      "938582        0  \n",
      "\n",
      "[842396 rows x 41 columns]\n",
      "\n",
      "=== Robust Scaled DataFrame ===\n",
      "        flow_time  header_size  packet_duration  overall_rate  src_rate  \\\n",
      "0        0.305697     1.244034              0.0      1.493555  1.493555   \n",
      "1        0.136455     0.927130              0.0      1.457053  1.457053   \n",
      "2       -0.002415     0.261344              0.0      0.193832  0.193832   \n",
      "3        0.787859     1.424523              0.0      1.459027  1.459027   \n",
      "4       -0.002415     0.235723              0.0     -0.416618 -0.416618   \n",
      "...           ...          ...              ...           ...       ...   \n",
      "938577   2.497585     1.789381              0.0      0.497081  0.497081   \n",
      "938578  -0.002415     0.000826              0.0      0.218864  0.218864   \n",
      "938580   1.954951     1.412376              0.0      2.066850  2.066850   \n",
      "938581   0.676725     1.384284              0.0      1.437256  1.437256   \n",
      "938582   2.497585     0.605417              0.0     -0.017292 -0.017292   \n",
      "\n",
      "        dst_rate  fin_packets  urg_packets  rst_packets  max_value  ...  \\\n",
      "0            0.0          0.0          0.0          0.0  -0.708840  ...   \n",
      "1            0.0          0.0          0.0          1.0   0.047671  ...   \n",
      "2            0.0          0.0          0.0          0.0   1.900958  ...   \n",
      "3            0.0          0.0          0.0          2.5   1.727209  ...   \n",
      "4            0.0          0.0          0.0          0.0   1.900958  ...   \n",
      "...          ...          ...          ...          ...        ...  ...   \n",
      "938577       0.0          0.0          0.0          0.0   1.900958  ...   \n",
      "938578       0.0          0.0          0.0          2.5   1.900958  ...   \n",
      "938580       0.0          0.0          0.0          0.0  -0.664767  ...   \n",
      "938581       0.0          0.0          0.0          0.0  -0.580866  ...   \n",
      "938582       0.0          0.0          0.0          2.5   0.659055  ...   \n",
      "\n",
      "        payload_entropy  value_range  flows_last_10s  unique_dsts_last_10s  \\\n",
      "0              0.063612    -0.708840       -1.000000                  -1.0   \n",
      "1              1.743042     0.047671       -0.333333                   1.5   \n",
      "2              0.856624     1.900958       -1.000000                   1.5   \n",
      "3              2.363465     1.727209        0.666667                  -0.5   \n",
      "4              0.426765     1.900958       -1.000000                  -0.5   \n",
      "...                 ...          ...             ...                   ...   \n",
      "938577         0.602562     1.900958       -0.333333                   0.0   \n",
      "938578         0.080281     1.900958       -0.333333                   0.5   \n",
      "938580         0.602322    -0.664767        0.000000                   0.0   \n",
      "938581         0.457823    -0.580866       -1.000000                   0.0   \n",
      "938582         0.057948     0.659055       -1.666667                   1.0   \n",
      "\n",
      "        hour_sin      hour_cos  handshake_complete  abrupt_reset  \\\n",
      "0      -0.683013 -1.830127e-01                   0             0   \n",
      "1       0.707107  1.731912e-16                   0             0   \n",
      "2      -0.183013 -6.830127e-01                   0             0   \n",
      "3      -0.683013 -1.830127e-01                   0             0   \n",
      "4       0.612372 -3.535534e-01                   0             0   \n",
      "...          ...           ...                 ...           ...   \n",
      "938577 -0.500000  5.000000e-01                   0             0   \n",
      "938578  0.500000  5.000000e-01                   0             0   \n",
      "938580  0.353553 -6.123724e-01                   0             0   \n",
      "938581 -0.707107  0.000000e+00                   0             0   \n",
      "938582 -0.500000 -5.000000e-01                   0             0   \n",
      "\n",
      "        tcp_syn_ratio  udp_psh  \n",
      "0                   0        0  \n",
      "1                   0        0  \n",
      "2                   0        0  \n",
      "3                   0        0  \n",
      "4                   0        0  \n",
      "...               ...      ...  \n",
      "938577              0        0  \n",
      "938578              0        0  \n",
      "938580              0        0  \n",
      "938581              0        0  \n",
      "938582              1        0  \n",
      "\n",
      "[842396 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "\n",
    "def scale_dataframe(df, scaler_type=\"minmax\", handshake_cols=None):\n",
    "    \"\"\"\n",
    "    Scale numeric features of a DataFrame using MinMaxScaler or RobustScaler.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame to scale.\n",
    "    scaler_type : {\"minmax\", \"robust\"}, default=\"minmax\"\n",
    "        Which scaler to use.\n",
    "    handshake_cols : list of str, optional\n",
    "        Columns to exclude from scaling in addition to binary columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A new DataFrame with selected columns scaled.\n",
    "    \"\"\"\n",
    "    # Default handshake-related columns to exclude\n",
    "    if handshake_cols is None:\n",
    "        handshake_cols = [\n",
    "            \"handshake_complete\",\n",
    "            \"abrupt_reset\",\n",
    "            \"tcp_syn_ratio\",\n",
    "            \"udp_psh\"\n",
    "        ]\n",
    "\n",
    "    # Choose scaler\n",
    "    scaler_type = scaler_type.lower()\n",
    "    if scaler_type == \"minmax\":\n",
    "        scaler = MinMaxScaler()\n",
    "    elif scaler_type == \"robust\":\n",
    "        scaler = RobustScaler()\n",
    "    else:\n",
    "        raise ValueError(\"scaler_type must be 'minmax' or 'robust'\")\n",
    "\n",
    "    # Identify binary columns (only 0/1 values)\n",
    "    binary_cols = [\n",
    "        col for col in df.columns\n",
    "        if df[col].dropna().isin([0, 1]).all() and df[col].dropna().unique().size <= 2\n",
    "    ]\n",
    "\n",
    "    # Determine which numeric columns to scale\n",
    "    exclude = set(binary_cols) | set(handshake_cols)\n",
    "    numeric_cols = df.select_dtypes(include=\"number\").columns\n",
    "    to_scale = [col for col in numeric_cols if col not in exclude]\n",
    "\n",
    "    # Fit and transform\n",
    "    scaled_vals = scaler.fit_transform(df[to_scale])\n",
    "    df_scaled = df.copy()\n",
    "    df_scaled[to_scale] = scaled_vals\n",
    "\n",
    "    return df_scaled\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # === Example 1: MinMaxScaler ===\n",
    "    # Scales selected features into the [0,1] range\n",
    "    df_minmax = scale_dataframe(enriched_data, scaler_type=\"minmax\")\n",
    "    print(\"=== MinMax Scaled DataFrame ===\")\n",
    "    print(df_minmax)\n",
    "\n",
    "    # === Example 2: RobustScaler ===\n",
    "    # Centers features on median and scales by IQR\n",
    "    df_robust = scale_dataframe(enriched_data, scaler_type=\"robust\")\n",
    "    print(\"\\n=== Robust Scaled DataFrame ===\")\n",
    "    print(df_robust)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d5528d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(842396, 41)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_minmax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65e005df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Desired output column order\n",
    "OUTPUT_ORDER = [\n",
    "    'flow_time', 'header_size', 'packet_duration', 'overall_rate',\n",
    "    'fin_packets', 'urg_packets', 'rst_packets', 'max_value',\n",
    "    'value_covariance', 'fin_flags', 'syn_flags', 'psh_flags', 'ack_flags',\n",
    "    'protocol_http', 'protocol_https', 'protocol_tcp', 'protocol_udp',\n",
    "    'protocol_icmp', 'label'\n",
    "]\n",
    "\n",
    "def select_features(df: pd.DataFrame, features: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retain only the specified features in the DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame from which to select features.\n",
    "    features : List[str]\n",
    "        List of feature names to retain.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A new DataFrame containing only the specified features (if present).\n",
    "    \"\"\"\n",
    "    # Intersect to avoid KeyErrors if some features are missing\n",
    "    available = [col for col in features if col in df.columns]\n",
    "    return df[available].copy()\n",
    "\n",
    "\n",
    "def filter_and_restore(\n",
    "    df_minmax: pd.DataFrame,\n",
    "    df_robust: pd.DataFrame,\n",
    "    features: List[str]\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Retain only the given features in both df_minmax and df_robust,\n",
    "    returning new DataFrames df_minmax2 and df_robust2, with columns\n",
    "    ordered according to OUTPUT_ORDER.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_minmax : pd.DataFrame\n",
    "        MinMax-scaled DataFrame.\n",
    "    df_robust : pd.DataFrame\n",
    "        Robust-scaled DataFrame.\n",
    "    features : List[str]\n",
    "        Features to keep.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_minmax2, df_robust2 : Tuple[pd.DataFrame, pd.DataFrame]\n",
    "        Filtered and re-ordered DataFrames containing only the specified features.\n",
    "    \"\"\"\n",
    "    # First, filter to the intersection of provided features\n",
    "    df_minmax2 = select_features(df_minmax, features)\n",
    "    df_robust2 = select_features(df_robust, features)\n",
    "\n",
    "    # Then, reindex columns to the desired output ordering\n",
    "    # This will include only those in OUTPUT_ORDER that are present\n",
    "    df_minmax2 = df_minmax2.reindex(columns=[col for col in OUTPUT_ORDER if col in df_minmax2.columns])\n",
    "    df_robust2 = df_robust2.reindex(columns=[col for col in OUTPUT_ORDER if col in df_robust2.columns])\n",
    "\n",
    "    return df_minmax2, df_robust2\n",
    "\n",
    "\n",
    "# Define the features to retain (should match or be a superset of OUTPUT_ORDER)\n",
    "selected_features = [\n",
    "    'flow_time', 'header_size', 'overall_rate', 'rst_packets', 'max_value',\n",
    "    'value_covariance', 'fin_flags', 'syn_flags', 'psh_flags', 'ack_flags',\n",
    "    'protocol_http', 'protocol_https', 'protocol_tcp', 'protocol_udp',\n",
    "    'protocol_icmp', 'packet_duration', 'fin_packets', 'urg_packets', 'label'\n",
    "]\n",
    "\n",
    "# Example usage:\n",
    "# Assuming df_minmax and df_robust are already defined DataFrames\n",
    "# df_minmax2, df_robust2 will contain only the selected features in the specified order\n",
    "\n",
    "df_minmax2, df_robust2 = filter_and_restore(df_minmax, df_robust, selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "694df25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flow_time', 'header_size', 'packet_duration', 'overall_rate',\n",
       "       'fin_packets', 'urg_packets', 'rst_packets', 'max_value',\n",
       "       'value_covariance', 'fin_flags', 'syn_flags', 'psh_flags', 'ack_flags',\n",
       "       'protocol_http', 'protocol_https', 'protocol_tcp', 'protocol_udp',\n",
       "       'protocol_icmp', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_minmax2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6df47ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flow_time', 'header_size', 'packet_duration', 'overall_rate',\n",
       "       'fin_packets', 'urg_packets', 'rst_packets', 'max_value',\n",
       "       'value_covariance', 'fin_flags', 'syn_flags', 'psh_flags', 'ack_flags',\n",
       "       'protocol_http', 'protocol_https', 'protocol_tcp', 'protocol_udp',\n",
       "       'protocol_icmp', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_robust2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f8c5a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Label Encoding Mapping ---\n",
      "'BenignTraffic' → 0\n",
      "'DDoS' → 1\n",
      "'DoS' → 2\n",
      "'MITM' → 3\n",
      "'Mirai' → 4\n",
      "'Recon' → 5\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- Label Encoding Mapping ---\n",
      "'BenignTraffic' → 0\n",
      "'DDoS' → 1\n",
      "'DoS' → 2\n",
      "'MITM' → 3\n",
      "'Mirai' → 4\n",
      "'Recon' → 5\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- Label Encoding Mapping ---\n",
      "'BenignTraffic' → 0\n",
      "'DDoS' → 1\n",
      "'DoS' → 2\n",
      "'MITM' → 3\n",
      "'Mirai' → 4\n",
      "'Recon' → 5\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- Label Encoding Mapping ---\n",
      "'BenignTraffic' → 0\n",
      "'DDoS' → 1\n",
      "'DoS' → 2\n",
      "'MITM' → 3\n",
      "'Mirai' → 4\n",
      "'Recon' → 5\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- Label Encoding Mapping ---\n",
      "'BenignTraffic' → 0\n",
      "'DDoS' → 1\n",
      "'DoS' → 2\n",
      "'MITM' → 3\n",
      "'Mirai' → 4\n",
      "'Recon' → 5\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import Tuple, Dict, Any\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def split_and_encode(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = \"label\"\n",
    ") -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Splits the DataFrame into features (X) and target (y),\n",
    "    applies label encoding to the target, and prints the encoding map.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame containing features and a target column.\n",
    "    target_col : str, default=\"label\"\n",
    "        The name of the target column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : pd.DataFrame\n",
    "        Feature matrix (all columns except the target).\n",
    "    y_encoded : pd.Series\n",
    "        Label-encoded target vector.\n",
    "    \"\"\"\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"Target column '{target_col}' not found in DataFrame.\")\n",
    "\n",
    "    # Step 1: Split features and target\n",
    "    X = df.drop(columns=target_col)\n",
    "    y = df[target_col]\n",
    "\n",
    "    # Step 2: Apply label encoding\n",
    "    le = LabelEncoder()\n",
    "    y_encoded_array = le.fit_transform(y)\n",
    "    y_encoded = pd.Series(y_encoded_array, name=target_col)\n",
    "\n",
    "    # Step 3: Show label encoding mapping\n",
    "    print(\"\\n--- Label Encoding Mapping ---\")\n",
    "    for i, label in enumerate(le.classes_):\n",
    "        print(f\"'{label}' → {i}\")\n",
    "    print(\"-----------------------------\\n\")\n",
    "\n",
    "    return X, y_encoded\n",
    "\n",
    "# Example usage:\n",
    "X_minmax, y_minmax = split_and_encode(df_minmax)\n",
    "X_minmax2, y_minmax2 = split_and_encode(df_minmax2)\n",
    "\n",
    "X, y = split_and_encode(enriched_data)\n",
    "\n",
    "X_robust,  y_robust  = split_and_encode(df_robust)\n",
    "X_robust2,  y_robust2  = split_and_encode(df_robust2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1f2b3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   flow_time  header_size  packet_duration  overall_rate  src_rate  dst_rate  \\\n",
      "0   0.123245     0.630603              0.0      0.790565  0.790565       0.0   \n",
      "1   0.055548     0.537041              0.0      0.777473  0.777473       0.0   \n",
      "2   0.000000     0.340476              0.0      0.324403  0.324403       0.0   \n",
      "3   0.316110     0.683890              0.0      0.778181  0.778181       0.0   \n",
      "4   0.000000     0.332912              0.0      0.105457  0.105457       0.0   \n",
      "\n",
      "   fin_packets  urg_packets  rst_packets  max_value  ...  payload_entropy  \\\n",
      "0          0.0          0.0          0.0   0.347550  ...         0.150044   \n",
      "1          0.0          0.0          0.4   0.536678  ...         0.484606   \n",
      "2          0.0          0.0          0.0   1.000000  ...         0.308021   \n",
      "3          0.0          0.0          1.0   0.956563  ...         0.608202   \n",
      "4          0.0          0.0          0.0   1.000000  ...         0.222388   \n",
      "\n",
      "   value_range  flows_last_10s  unique_dsts_last_10s  hour_sin  hour_cos  \\\n",
      "0     0.347550        0.105263              0.071429  0.017037  0.370590   \n",
      "1     0.536678        0.210526              0.428571  1.000000  0.500000   \n",
      "2     1.000000        0.105263              0.428571  0.370590  0.017037   \n",
      "3     0.956563        0.368421              0.142857  0.017037  0.370590   \n",
      "4     1.000000        0.105263              0.142857  0.933013  0.250000   \n",
      "\n",
      "   handshake_complete  abrupt_reset  tcp_syn_ratio  udp_psh  \n",
      "0                   0             0              0        0  \n",
      "1                   0             0              0        0  \n",
      "2                   0             0              0        0  \n",
      "3                   0             0              0        0  \n",
      "4                   0             0              0        0  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "0    1\n",
      "1    1\n",
      "2    2\n",
      "3    2\n",
      "4    2\n",
      "Name: label, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "print(X_minmax.head())\n",
    "print(y_minmax.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a6548aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   flow_time  header_size  packet_duration  overall_rate  fin_packets  \\\n",
      "0   0.123245     0.630603              0.0      0.790565          0.0   \n",
      "1   0.055548     0.537041              0.0      0.777473          0.0   \n",
      "2   0.000000     0.340476              0.0      0.324403          0.0   \n",
      "3   0.316110     0.683890              0.0      0.778181          0.0   \n",
      "4   0.000000     0.332912              0.0      0.105457          0.0   \n",
      "\n",
      "   urg_packets  rst_packets  max_value  value_covariance  fin_flags  \\\n",
      "0          0.0          0.0   0.347550          0.000000          0   \n",
      "1          0.0          0.4   0.536678          0.768808          0   \n",
      "2          0.0          0.0   1.000000          0.000000          0   \n",
      "3          0.0          1.0   0.956563          1.000000          0   \n",
      "4          0.0          0.0   1.000000          0.000000          0   \n",
      "\n",
      "   syn_flags  psh_flags  ack_flags  protocol_http  protocol_https  \\\n",
      "0          0          0          0              0               0   \n",
      "1          0          0          0              0               0   \n",
      "2          0          0          0              0               0   \n",
      "3          0          0          0              0               0   \n",
      "4          0          0          0              0               0   \n",
      "\n",
      "   protocol_tcp  protocol_udp  protocol_icmp  \n",
      "0             0             1              0  \n",
      "1             0             1              0  \n",
      "2             0             1              0  \n",
      "3             0             1              0  \n",
      "4             0             1              0  \n",
      "0    1\n",
      "1    1\n",
      "2    2\n",
      "3    2\n",
      "4    2\n",
      "Name: label, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "print(X_minmax2.head())\n",
    "print(y_minmax2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de362fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   flow_time  header_size  packet_duration  overall_rate  src_rate  dst_rate  \\\n",
      "0   0.305697     1.244034              0.0      1.493555  1.493555       0.0   \n",
      "1   0.136455     0.927130              0.0      1.457053  1.457053       0.0   \n",
      "2  -0.002415     0.261344              0.0      0.193832  0.193832       0.0   \n",
      "3   0.787859     1.424523              0.0      1.459027  1.459027       0.0   \n",
      "4  -0.002415     0.235723              0.0     -0.416618 -0.416618       0.0   \n",
      "\n",
      "   fin_packets  urg_packets  rst_packets  max_value  ...  payload_entropy  \\\n",
      "0          0.0          0.0          0.0  -0.708840  ...         0.063612   \n",
      "1          0.0          0.0          1.0   0.047671  ...         1.743042   \n",
      "2          0.0          0.0          0.0   1.900958  ...         0.856624   \n",
      "3          0.0          0.0          2.5   1.727209  ...         2.363465   \n",
      "4          0.0          0.0          0.0   1.900958  ...         0.426765   \n",
      "\n",
      "   value_range  flows_last_10s  unique_dsts_last_10s  hour_sin      hour_cos  \\\n",
      "0    -0.708840       -1.000000                  -1.0 -0.683013 -1.830127e-01   \n",
      "1     0.047671       -0.333333                   1.5  0.707107  1.731912e-16   \n",
      "2     1.900958       -1.000000                   1.5 -0.183013 -6.830127e-01   \n",
      "3     1.727209        0.666667                  -0.5 -0.683013 -1.830127e-01   \n",
      "4     1.900958       -1.000000                  -0.5  0.612372 -3.535534e-01   \n",
      "\n",
      "   handshake_complete  abrupt_reset  tcp_syn_ratio  udp_psh  \n",
      "0                   0             0              0        0  \n",
      "1                   0             0              0        0  \n",
      "2                   0             0              0        0  \n",
      "3                   0             0              0        0  \n",
      "4                   0             0              0        0  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "0    1\n",
      "1    1\n",
      "2    2\n",
      "3    2\n",
      "4    2\n",
      "Name: label, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "print(X_robust.head())\n",
    "print(y_robust.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19fd37d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   flow_time  header_size  packet_duration  overall_rate  fin_packets  \\\n",
      "0   0.305697     1.244034              0.0      1.493555          0.0   \n",
      "1   0.136455     0.927130              0.0      1.457053          0.0   \n",
      "2  -0.002415     0.261344              0.0      0.193832          0.0   \n",
      "3   0.787859     1.424523              0.0      1.459027          0.0   \n",
      "4  -0.002415     0.235723              0.0     -0.416618          0.0   \n",
      "\n",
      "   urg_packets  rst_packets  max_value  value_covariance  fin_flags  \\\n",
      "0          0.0          0.0  -0.708840           0.00000          0   \n",
      "1          0.0          1.0   0.047671           1.92202          0   \n",
      "2          0.0          0.0   1.900958           0.00000          0   \n",
      "3          0.0          2.5   1.727209           2.50000          0   \n",
      "4          0.0          0.0   1.900958           0.00000          0   \n",
      "\n",
      "   syn_flags  psh_flags  ack_flags  protocol_http  protocol_https  \\\n",
      "0          0          0          0              0               0   \n",
      "1          0          0          0              0               0   \n",
      "2          0          0          0              0               0   \n",
      "3          0          0          0              0               0   \n",
      "4          0          0          0              0               0   \n",
      "\n",
      "   protocol_tcp  protocol_udp  protocol_icmp  \n",
      "0             0             1              0  \n",
      "1             0             1              0  \n",
      "2             0             1              0  \n",
      "3             0             1              0  \n",
      "4             0             1              0  \n",
      "0    1\n",
      "1    1\n",
      "2    2\n",
      "3    2\n",
      "4    2\n",
      "Name: label, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "print(X_robust2.head())\n",
    "print(y_robust2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb751327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_train_test(X, y, test_size=0.3, random_state=None, shuffle=True):\n",
    "    \"\"\"\n",
    "    Split features X and target y into train and test sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like or DataFrame\n",
    "        Feature matrix.\n",
    "    y : array-like or Series\n",
    "        Target vector.\n",
    "    test_size : float or int, default=0.3\n",
    "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
    "        of the dataset to include in the test split.\n",
    "    random_state : int or None, default=None\n",
    "        Controls the shuffling for reproducibility.\n",
    "    shuffle : bool, default=True\n",
    "        Whether or not to shuffle the data before splitting.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Suppose you have already created:\n",
    "#   X_minmax, y_minmax   # features & target scaled with MinMaxScaler\n",
    "#   X_robust,  y_robust  # features & target scaled with RobustScaler\n",
    "\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = split_train_test(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# 1) Split the MinMax-scaled data\n",
    "X_train_mm, X_test_mm, y_train_mm, y_test_mm = split_train_test(\n",
    "    X_minmax, \n",
    "    y_minmax, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train_mm2, X_test_mm2, y_train_mm2, y_test_mm2 = split_train_test(\n",
    "    X_minmax2, \n",
    "    y_minmax2, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# 2) Split the Robust-scaled data\n",
    "X_train_rb, X_test_rb, y_train_rb, y_test_rb = split_train_test(\n",
    "    X_robust, \n",
    "    y_robust, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train_rb2, X_test_rb2, y_train_rb2, y_test_rb2 = split_train_test(\n",
    "    X_robust2, \n",
    "    y_robust2, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# You now have:\n",
    "#  - X_train_mm, y_train_mm for training on MinMax data\n",
    "#  - X_test_mm,  y_test_mm  for testing  on MinMax data\n",
    "#  - X_train_rb, y_train_rb for training on Robust data\n",
    "#  - X_test_rb,  y_test_rb  for testing  on Robust data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d4e07e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "708705    1\n",
       "730742    1\n",
       "324370    4\n",
       "658629    1\n",
       "447432    1\n",
       "         ..\n",
       "259178    1\n",
       "365838    1\n",
       "131932    1\n",
       "671155    2\n",
       "121958    1\n",
       "Name: label, Length: 673916, dtype: int32"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_mm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bef139a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "708705    1\n",
       "730742    1\n",
       "324370    4\n",
       "658629    1\n",
       "447432    1\n",
       "         ..\n",
       "259178    1\n",
       "365838    1\n",
       "131932    1\n",
       "671155    2\n",
       "121958    1\n",
       "Name: label, Length: 673916, dtype: int32"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa694258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining features after Variance Thresholding: 32\n",
      "Remaining features after Variance Thresholding: 32\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "def variance_threshold_selector(X_train: pd.DataFrame,\n",
    "                                X_test: pd.DataFrame,\n",
    "                                threshold: float = 0.01):\n",
    "    \"\"\"\n",
    "    Fit a VarianceThreshold on X_train and apply it to both X_train and X_test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : pd.DataFrame\n",
    "        Training feature matrix.\n",
    "    X_test : pd.DataFrame\n",
    "        Test feature matrix.\n",
    "    threshold : float, default=0.01\n",
    "        Features with variance <= threshold will be removed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train_sel : pd.DataFrame\n",
    "        X_train reduced to only the features with variance > threshold.\n",
    "    X_test_sel : pd.DataFrame\n",
    "        X_test reduced to the same set of selected features.\n",
    "    selected_cols : pd.Index\n",
    "        Names of the retained features.\n",
    "    \"\"\"\n",
    "    # 1. Fit on train\n",
    "    vt = VarianceThreshold(threshold=threshold)\n",
    "    X_train_arr = vt.fit_transform(X_train)\n",
    "    selected_cols = X_train.columns[vt.get_support()]\n",
    "\n",
    "    # 2. Build clean DataFrame for train\n",
    "    X_train_sel = pd.DataFrame(\n",
    "        X_train_arr,\n",
    "        columns=selected_cols,\n",
    "        index=X_train.index\n",
    "    )\n",
    "\n",
    "    # 3. Filter test to only those columns (if they exist)\n",
    "    common_cols = [c for c in selected_cols if c in X_test.columns]\n",
    "    X_test_sel = X_test.loc[:, common_cols].copy()\n",
    "\n",
    "    print(f\"Remaining features after Variance Thresholding: {len(selected_cols)}\")\n",
    "    return X_train_sel, X_test_sel, selected_cols\n",
    "\n",
    "\n",
    "# On MinMax-scaled data\n",
    "X_train_mm_sel, X_test_mm_sel, mm_cols = variance_threshold_selector(\n",
    "    X_train_mm,\n",
    "    X_test_mm,\n",
    "    threshold=0.01\n",
    ")\n",
    "\n",
    "# On Robust-scaled data\n",
    "X_train_rb_sel, X_test_rb_sel, rb_cols = variance_threshold_selector(\n",
    "    X_train_rb,\n",
    "    X_test_rb,\n",
    "    threshold=0.01\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbe936e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(673916, 32)\n",
      "(168480, 32)\n",
      "Index(['flow_time', 'header_size', 'overall_rate', 'src_rate', 'rst_packets',\n",
      "       'max_value', 'value_covariance', 'fin_flags', 'syn_flags', 'rst_flags',\n",
      "       'psh_flags', 'ack_flags', 'protocol_http', 'protocol_https',\n",
      "       'protocol_tcp', 'protocol_udp', 'protocol_icmp', 'rate_ratio',\n",
      "       'rst_to_fin', 'avg_pkt_size', 'mean_interpkt', 'std_interpkt',\n",
      "       'p90_interpkt', 'max_interpkt', 'burstiness', 'payload_entropy',\n",
      "       'value_range', 'flows_last_10s', 'unique_dsts_last_10s', 'hour_sin',\n",
      "       'hour_cos', 'tcp_syn_ratio'],\n",
      "      dtype='object')\n",
      "(673916, 32)\n",
      "(168480, 32)\n",
      "Index(['flow_time', 'header_size', 'overall_rate', 'src_rate', 'rst_packets',\n",
      "       'max_value', 'value_covariance', 'fin_flags', 'syn_flags', 'rst_flags',\n",
      "       'psh_flags', 'ack_flags', 'protocol_http', 'protocol_https',\n",
      "       'protocol_tcp', 'protocol_udp', 'protocol_icmp', 'rate_ratio',\n",
      "       'rst_to_fin', 'avg_pkt_size', 'mean_interpkt', 'std_interpkt',\n",
      "       'p90_interpkt', 'max_interpkt', 'burstiness', 'payload_entropy',\n",
      "       'value_range', 'flows_last_10s', 'unique_dsts_last_10s', 'hour_sin',\n",
      "       'hour_cos', 'tcp_syn_ratio'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X_train_mm_sel.shape)\n",
    "print(X_test_mm_sel.shape)\n",
    "print(mm_cols)\n",
    "\n",
    "\n",
    "print(X_train_rb_sel.shape)\n",
    "print(X_test_rb_sel.shape)\n",
    "print(rb_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a4ebcbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flow_time', 'header_size', 'overall_rate', 'src_rate', 'rst_packets',\n",
       "       'max_value', 'value_covariance', 'fin_flags', 'syn_flags', 'rst_flags',\n",
       "       'psh_flags', 'ack_flags', 'protocol_http', 'protocol_https',\n",
       "       'protocol_tcp', 'protocol_udp', 'protocol_icmp', 'rate_ratio',\n",
       "       'rst_to_fin', 'avg_pkt_size', 'mean_interpkt', 'std_interpkt',\n",
       "       'p90_interpkt', 'max_interpkt', 'burstiness', 'payload_entropy',\n",
       "       'value_range', 'flows_last_10s', 'unique_dsts_last_10s', 'hour_sin',\n",
       "       'hour_cos', 'tcp_syn_ratio'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mm_sel.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3d20f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flow_time', 'header_size', 'overall_rate', 'src_rate', 'rst_packets',\n",
       "       'max_value', 'value_covariance', 'fin_flags', 'syn_flags', 'rst_flags',\n",
       "       'psh_flags', 'ack_flags', 'protocol_http', 'protocol_https',\n",
       "       'protocol_tcp', 'protocol_udp', 'protocol_icmp', 'rate_ratio',\n",
       "       'rst_to_fin', 'avg_pkt_size', 'mean_interpkt', 'std_interpkt',\n",
       "       'p90_interpkt', 'max_interpkt', 'burstiness', 'payload_entropy',\n",
       "       'value_range', 'flows_last_10s', 'unique_dsts_last_10s', 'hour_sin',\n",
       "       'hour_cos', 'tcp_syn_ratio'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_rb_sel.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9855aa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed redundant features (correlation-based): ['src_rate', 'rst_flags', 'rate_ratio', 'rst_to_fin', 'mean_interpkt', 'std_interpkt', 'p90_interpkt', 'max_interpkt', 'value_range', 'tcp_syn_ratio']\n",
      "Final number of features: 22\n",
      "Removed redundant features (correlation-based): ['src_rate', 'rst_flags', 'rate_ratio', 'rst_to_fin', 'mean_interpkt', 'std_interpkt', 'p90_interpkt', 'max_interpkt', 'value_range', 'tcp_syn_ratio']\n",
      "Final number of features: 22\n"
     ]
    }
   ],
   "source": [
    "def correlation_filter(X_train: pd.DataFrame,\n",
    "                       X_test: pd.DataFrame,\n",
    "                       threshold: float = 0.95):\n",
    "    \"\"\"\n",
    "    Remove features in X_train and X_test that are highly correlated (> threshold).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : pd.DataFrame\n",
    "        Training feature matrix (after any prior filtering/scaling).\n",
    "    X_test : pd.DataFrame\n",
    "        Test feature matrix (after any prior filtering/scaling).\n",
    "    threshold : float, default=0.95\n",
    "        Upper bound on allowed pairwise correlation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train_sel : pd.DataFrame\n",
    "        X_train with correlated features removed.\n",
    "    X_test_sel : pd.DataFrame\n",
    "        X_test with the same features removed (where present).\n",
    "    dropped_cols : List[str]\n",
    "        Names of features dropped from both.\n",
    "    \"\"\"\n",
    "    # 1. compute pairwise abs-correlation\n",
    "    corr_matrix = X_train.corr().abs()\n",
    "    # 2. take upper triangle (exclude diagonal)\n",
    "    upper_tri = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    # 3. find columns with any correlation above threshold\n",
    "    dropped_cols = [\n",
    "        col for col in upper_tri.columns\n",
    "        if any(upper_tri[col] > threshold)\n",
    "    ]\n",
    "    # 4. drop them\n",
    "    X_train_sel = X_train.drop(columns=dropped_cols)\n",
    "    X_test_sel  = X_test.drop(columns=dropped_cols, errors='ignore')\n",
    "    # 5. keep only the intersection (in case some cols missing in test)\n",
    "    common = X_train_sel.columns.intersection(X_test_sel.columns)\n",
    "    X_train_sel = X_train_sel[common].reset_index(drop=True)\n",
    "    X_test_sel  = X_test_sel[common].reset_index(drop=True)\n",
    "\n",
    "    print(\"Removed redundant features (correlation-based):\", dropped_cols)\n",
    "    print(f\"Final number of features: {X_train_sel.shape[1]}\")\n",
    "\n",
    "    return X_train_sel, X_test_sel, dropped_cols\n",
    "\n",
    "\n",
    "# 1) On the MinMax pipeline\n",
    "X_train_mm_corr, X_test_mm_corr, mm_dropped = correlation_filter(\n",
    "    X_train_mm_sel,\n",
    "    X_test_mm_sel,\n",
    "    threshold=0.95\n",
    ")\n",
    "\n",
    "# 2) On the Robust pipeline\n",
    "X_train_rb_corr, X_test_rb_corr, rb_dropped = correlation_filter(\n",
    "    X_train_rb_sel,\n",
    "    X_test_rb_sel,\n",
    "    threshold=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1bf71430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(673916, 22)\n",
      "(168480, 22)\n",
      "['src_rate', 'rst_flags', 'rate_ratio', 'rst_to_fin', 'mean_interpkt', 'std_interpkt', 'p90_interpkt', 'max_interpkt', 'value_range', 'tcp_syn_ratio']\n",
      "(673916, 22)\n",
      "(168480, 22)\n",
      "['src_rate', 'rst_flags', 'rate_ratio', 'rst_to_fin', 'mean_interpkt', 'std_interpkt', 'p90_interpkt', 'max_interpkt', 'value_range', 'tcp_syn_ratio']\n"
     ]
    }
   ],
   "source": [
    "print(X_train_mm_corr.shape)\n",
    "print(X_test_mm_corr.shape)\n",
    "print(mm_dropped)\n",
    "\n",
    "print(X_train_rb_corr.shape)\n",
    "print(X_test_rb_corr.shape)\n",
    "print(rb_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b6d8293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flow_time', 'header_size', 'overall_rate', 'rst_packets', 'max_value',\n",
       "       'value_covariance', 'fin_flags', 'syn_flags', 'psh_flags', 'ack_flags',\n",
       "       'protocol_http', 'protocol_https', 'protocol_tcp', 'protocol_udp',\n",
       "       'protocol_icmp', 'avg_pkt_size', 'burstiness', 'payload_entropy',\n",
       "       'flows_last_10s', 'unique_dsts_last_10s', 'hour_sin', 'hour_cos'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mm_corr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8289b3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flow_time', 'header_size', 'overall_rate', 'rst_packets', 'max_value',\n",
       "       'value_covariance', 'fin_flags', 'syn_flags', 'psh_flags', 'ack_flags',\n",
       "       'protocol_http', 'protocol_https', 'protocol_tcp', 'protocol_udp',\n",
       "       'protocol_icmp', 'avg_pkt_size', 'burstiness', 'payload_entropy',\n",
       "       'flows_last_10s', 'unique_dsts_last_10s', 'hour_sin', 'hour_cos'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_rb_corr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e89232e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['flow_time', 'header_size', 'packet_duration', 'overall_rate',\n",
       "       'fin_packets', 'urg_packets', 'rst_packets', 'max_value',\n",
       "       'value_covariance', 'fin_flags', 'syn_flags', 'psh_flags', 'ack_flags',\n",
       "       'protocol_http', 'protocol_https', 'protocol_tcp', 'protocol_udp',\n",
       "       'protocol_icmp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mm2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c17b6d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "708705    1\n",
       "730742    1\n",
       "324370    4\n",
       "658629    1\n",
       "447432    1\n",
       "779184    1\n",
       "22042     1\n",
       "440408    1\n",
       "126887    2\n",
       "339675    1\n",
       "148152    1\n",
       "308277    1\n",
       "368504    1\n",
       "586644    1\n",
       "66006     1\n",
       "314886    1\n",
       "696549    5\n",
       "269164    1\n",
       "286210    1\n",
       "313950    1\n",
       "99522     1\n",
       "430435    2\n",
       "804044    2\n",
       "416781    2\n",
       "122551    1\n",
       "236623    1\n",
       "449322    1\n",
       "209014    1\n",
       "785747    1\n",
       "125008    1\n",
       "Name: label, dtype: int32"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_mm2.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1b2374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "# from sklearn.utils import shuffle\n",
    "\n",
    "# def smote_oversample(X_train: pd.DataFrame,\n",
    "#                      y_train: pd.Series,\n",
    "#                      random_state: int = 42):\n",
    "#     \"\"\"\n",
    "#     Apply SMOTE to balance classes in the training set.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     X_train : pd.DataFrame\n",
    "#         Feature matrix for training.\n",
    "#     y_train : pd.Series\n",
    "#         Target vector for training.\n",
    "#     random_state : int, default=42\n",
    "#         Seed for reproducibility.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     X_resampled : pd.DataFrame\n",
    "#         SMOTE-resampled feature matrix.\n",
    "#     y_resampled : pd.Series\n",
    "#         SMOTE-resampled target vector.\n",
    "#     \"\"\"\n",
    "#     # 1. Fit SMOTE on the training data\n",
    "#     smote = SMOTE(random_state=random_state)\n",
    "#     X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "#     # 2. Shuffle the resampled data to mix original and synthetic samples\n",
    "#     X_res, y_res = shuffle(X_res, y_res, random_state=random_state)\n",
    "\n",
    "#     # 3. Convert back to DataFrame/Series (if needed)\n",
    "#     if not isinstance(X_res, pd.DataFrame):\n",
    "#         X_res = pd.DataFrame(X_res, columns=X_train.columns)\n",
    "#     if not isinstance(y_res, pd.Series):\n",
    "#         y_res = pd.Series(y_res, name=y_train.name)\n",
    "\n",
    "#     print(f\"After SMOTE, training set size: {X_res.shape[0]} samples\")\n",
    "#     return X_res, y_res\n",
    "\n",
    "# # --- Example usage ---\n",
    "\n",
    "# # 1) On the MinMax pipeline\n",
    "\n",
    "# X_train_mm_res, y_train_mm_res = smote_oversample(\n",
    "#     X_train_mm_corr,\n",
    "#     y_train_mm,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# X_train_mm_res2, y_train_mm_res2 = smote_oversample(\n",
    "#     X_train_mm2,\n",
    "#     y_train_mm2,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "\n",
    "# # 2) On the Robust pipeline\n",
    "\n",
    "# X_train_rb_res, y_train_rb_res = smote_oversample(\n",
    "#     X_train_rb_corr,\n",
    "#     y_train_rb,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# X_train_rb_res2, y_train_rb_res2 = smote_oversample(\n",
    "#     X_train_rb2,\n",
    "#     y_train_rb2,\n",
    "#     random_state=42\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24ff3ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs = [\n",
    "#     (\"MinMax, corr-filtered\", X_train_mm_corr, y_train_mm, X_train_mm_res,  y_train_mm_res),\n",
    "#     (\"MinMax, second split\",  X_train_mm2,       y_train_mm2, X_train_mm_res2, y_train_mm_res2),\n",
    "#     (\"Robust, corr-filtered\", X_train_rb_corr, y_train_rb,   X_train_rb_res,  y_train_rb_res),\n",
    "#     (\"Robust, second split\",  X_train_rb2,       y_train_rb2, X_train_rb_res2, y_train_rb_res2),\n",
    "# ]\n",
    "\n",
    "# for name, X_bef, y_bef, X_aft, y_aft in runs:\n",
    "#     print(f\"=== {name} ===\")\n",
    "#     print(f\"Original training shape: {X_bef.shape}\")\n",
    "#     print(f\"Resampled training shape: {X_aft.shape}\\n\")\n",
    "\n",
    "#     print(\"Class distribution before SMOTE:\")\n",
    "#     print(pd.Series(y_bef.values.ravel()).value_counts(), \"\\n\")\n",
    "\n",
    "#     print(\"Class distribution after SMOTE:\")\n",
    "#     print(pd.Series(y_aft.values.ravel()).value_counts(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f197e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 1: MinMaxScaler (my chosen columns)\n",
    "\n",
    "# # Imports\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12d7fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. XGBoost\n",
    "# xgb_mm = XGBClassifier(random_state=42)\n",
    "# xgb_mm.fit(X_train_mm_res, y_train_mm_res)\n",
    "\n",
    "# # Evaluate\n",
    "# y_pred_xgb_mm = xgb_mm.predict(X_test_mm_corr)\n",
    "# acc_xgb_mm = accuracy_score(y_test_mm, y_pred_xgb_mm)\n",
    "# print(f\"XGBoost (MinMax my cols) Accuracy: {acc_xgb_mm:.4f}\")\n",
    "\n",
    "# # Save XGBoost model\n",
    "# joblib.dump(xgb_mm, 'xgb_mm_mycols.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66475512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. Random Forest\n",
    "# rf_mm = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf_mm.fit(X_train_mm_res, y_train_mm_res)\n",
    "\n",
    "# # Evaluate\n",
    "# y_pred_rf_mm = rf_mm.predict(X_test_mm_corr)\n",
    "# acc_rf_mm = accuracy_score(y_test_mm, y_pred_rf_mm)\n",
    "# print(f\"Random Forest (MinMax my cols) Accuracy: {acc_rf_mm:.4f}\")\n",
    "\n",
    "# # Save Random Forest model\n",
    "# joblib.dump(rf_mm, 'rf_mm_mycols.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c60e3071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 2: MinMaxScaler (your chosen columns)\n",
    "\n",
    "# # 1. XGBoost\n",
    "# xgb_mm2 = XGBClassifier(random_state=42)\n",
    "# xgb_mm2.fit(X_train_mm_res2, y_train_mm_res2)\n",
    "\n",
    "# # Evaluate\n",
    "# y_pred_xgb_mm2 = xgb_mm2.predict(X_test_mm2)\n",
    "# acc_xgb_mm2 = accuracy_score(y_test_mm2, y_pred_xgb_mm2)\n",
    "# print(f\"XGBoost (MinMax your cols) Accuracy: {acc_xgb_mm2:.4f}\")\n",
    "\n",
    "# # Save XGBoost model\n",
    "# joblib.dump(xgb_mm2, 'xgb_mm_yourcols.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b065ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. Random Forest\n",
    "# rf_mm2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf_mm2.fit(X_train_mm_res2, y_train_mm_res2)\n",
    "\n",
    "# # Evaluate\n",
    "# y_pred_rf_mm2 = rf_mm2.predict(X_test_mm2)\n",
    "# acc_rf_mm2 = accuracy_score(y_test_mm2, y_pred_rf_mm2)\n",
    "# print(f\"Random Forest (MinMax your cols) Accuracy: {acc_rf_mm2:.4f}\")\n",
    "\n",
    "# # Save Random Forest model\n",
    "# joblib.dump(rf_mm2, 'rf_mm_yourcols.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "21e3b2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 3: RobustScaler (my chosen columns)\n",
    "\n",
    "# # Imports\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import joblib\n",
    "\n",
    "# # Dataset splits\n",
    "# # X_train_rb_res, X_test_rb_corr, y_train_rb_res, y_test_rb\n",
    "\n",
    "# # 1. XGBoost\n",
    "# xgb_rb = XGBClassifier(random_state=42)\n",
    "# xgb_rb.fit(X_train_rb_res, y_train_rb_res)\n",
    "\n",
    "# # Evaluate\n",
    "# y_pred_xgb_rb = xgb_rb.predict(X_test_rb_corr)\n",
    "# acc_xgb_rb = accuracy_score(y_test_rb, y_pred_xgb_rb)\n",
    "# print(f\"XGBoost (Robust my cols) Accuracy: {acc_xgb_rb:.4f}\")\n",
    "\n",
    "# # Save XGBoost model\n",
    "# joblib.dump(xgb_rb, 'xgb_rb_mycols.joblib')\n",
    "\n",
    "# # 2. Random Forest\n",
    "# rf_rb = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf_rb.fit(X_train_rb_res, y_train_rb_res)\n",
    "\n",
    "# # Evaluate\n",
    "# y_pred_rf_rb = rf_rb.predict(X_test_rb_corr)\n",
    "# acc_rf_rb = accuracy_score(y_test_rb, y_pred_rf_rb)\n",
    "# print(f\"Random Forest (Robust my cols) Accuracy: {acc_rf_rb:.4f}\")\n",
    "\n",
    "# # Save Random Forest model\n",
    "# joblib.dump(rf_rb, 'rf_rb_mycols.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f5b0fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 4: RobustScaler (your chosen columns)\n",
    "\n",
    "# # Imports\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import joblib\n",
    "\n",
    "# # Dataset splits\n",
    "# # X_train_rb_res2, X_test_rb2, y_train_rb_res2, y_test_rb2\n",
    "\n",
    "# # 1. XGBoost\n",
    "# xgb_rb2 = XGBClassifier(random_state=42)\n",
    "# xgb_rb2.fit(X_train_rb_res2, y_train_rb_res2)\n",
    "\n",
    "# # Evaluate\n",
    "# y_pred_xgb_rb2 = xgb_rb2.predict(X_test_rb2)\n",
    "# acc_xgb_rb2 = accuracy_score(y_test_rb2, y_pred_xgb_rb2)\n",
    "# print(f\"XGBoost (Robust your cols) Accuracy: {acc_xgb_rb2:.4f}\")\n",
    "\n",
    "# # Save XGBoost model\n",
    "# joblib.dump(xgb_rb2, 'xgb_rb_yourcols.joblib')\n",
    "\n",
    "# # 2. Random Forest\n",
    "# rf_rb2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf_rb2.fit(X_train_rb_res2, y_train_rb_res2)\n",
    "\n",
    "# # Evaluate\n",
    "# y_pred_rf_rb2 = rf_rb2.predict(X_test_rb2)\n",
    "# acc_rf_rb2 = accuracy_score(y_test_rb2, y_pred_rf_rb2)\n",
    "# print(f\"Random Forest (Robust your cols) Accuracy: {acc_rf_rb2:.4f}\")\n",
    "\n",
    "# # Save Random Forest model\n",
    "# joblib.dump(rf_rb2, 'rf_rb_yourcols.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1cb086ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 53\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# ----------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# 3. Cross-validate on the RAW (imbalanced) train set\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# ----------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     52\u001b[0m skf \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mRANDOM_STATE)\n\u001b[1;32m---> 53\u001b[0m cv_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     60\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCV accuracies: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcv_scores\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean CV accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcv_scores\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_validation.py:684\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    682\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 684\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_validation.py:411\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    410\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 411\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    431\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. ASSUMPTION: You already have your raw, un-resampled datasets\n",
    "#    X_train_raw, X_test_raw, y_train_raw, y_test_raw\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Ensure reproducibility and output directory\n",
    "RANDOM_STATE = 42\n",
    "save_dir = \"saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Number of classes (for XGB)\n",
    "classes = np.unique(y_train_raw)\n",
    "num_classes = len(classes)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2. Build an imbalanced-safe pipeline: scaling, SMOTE, then XGB (GPU-enabled)\n",
    "# ----------------------------------------------------------------------------\n",
    "pipeline = ImbPipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote',  SMOTE(random_state=RANDOM_STATE)),\n",
    "    ('clf',    xgb.XGBClassifier(\n",
    "        objective='multi:softmax',\n",
    "        num_class=num_classes,\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=1000,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=0.5,\n",
    "        tree_method='gpu_hist',        # GPU training method\n",
    "        predictor='gpu_predictor',     # GPU predictor\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbosity=1,\n",
    "        eval_metric='merror'           # error-rate for multi-class\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 3. Cross-validate on the RAW (imbalanced) train set\n",
    "# ----------------------------------------------------------------------------\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "cv_scores = cross_val_score(\n",
    "    pipeline,\n",
    "    X_train_raw,\n",
    "    y_train_raw,\n",
    "    cv=skf,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(f\"CV accuracies: {cv_scores}\")\n",
    "print(f\"Mean CV accuracy: {cv_scores.mean():.4f}\\n\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 4. Final fit WITH early stopping: carve out a small validation split\n",
    "# ----------------------------------------------------------------------------\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train_raw, y_train_raw,\n",
    "    test_size=0.1,\n",
    "    stratify=y_train_raw,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Inject early stopping params into the pipeline's XGBClassifier\n",
    "pipeline.set_params(\n",
    "    clf__early_stopping_rounds=50,\n",
    "    clf__eval_set=[(X_val, y_val)],\n",
    "    clf__verbose=False\n",
    ")\n",
    "\n",
    "# Train on the reduced train set (scaling + SMOTE applied inside)\n",
    "pipeline.fit(X_tr, y_tr)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 5. Evaluate on the untouched test set\n",
    "# ----------------------------------------------------------------------------\n",
    "y_test_pred = pipeline.predict(X_test_raw)\n",
    "print(f\"Test accuracy: {accuracy_score(y_test_raw, y_test_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_raw, y_test_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_raw, y_test_pred))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 6. Save the full pipeline for later inference\n",
    "# ----------------------------------------------------------------------------\n",
    "model_path = os.path.join(save_dir, 'xgb_pipeline_gpu.joblib')\n",
    "joblib.dump(pipeline, model_path)\n",
    "print(f\"✔ Pipeline saved to {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

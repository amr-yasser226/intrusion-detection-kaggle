{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "721fe110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process files for feature engineering and scaling...\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Direct_Removal\\feature_added_datasets\\phase2_Direct_Removal_X_train_DirectRemoval_with_features.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Direct_Removal\\scaled_datasets\\phase2_Direct_Removal_X_train_DirectRemoval_StandardScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Direct_Removal\\scaled_datasets\\phase2_Direct_Removal_X_train_DirectRemoval_MinMaxScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Direct_Removal\\scaled_datasets\\phase2_Direct_Removal_X_train_DirectRemoval_RobustScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Direct_Removal\\scaled_datasets\\phase2_Direct_Removal_X_train_DirectRemoval_PowerTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Direct_Removal\\scaled_datasets\\phase2_Direct_Removal_X_train_DirectRemoval_QuantileTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Isolation_Forest\\feature_added_datasets\\phase2_Direct_Removal_X_train_IsolationForest_with_features.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Isolation_Forest\\scaled_datasets\\phase2_Direct_Removal_X_train_IsolationForest_StandardScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Isolation_Forest\\scaled_datasets\\phase2_Direct_Removal_X_train_IsolationForest_MinMaxScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Isolation_Forest\\scaled_datasets\\phase2_Direct_Removal_X_train_IsolationForest_RobustScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Isolation_Forest\\scaled_datasets\\phase2_Direct_Removal_X_train_IsolationForest_PowerTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Isolation_Forest\\scaled_datasets\\phase2_Direct_Removal_X_train_IsolationForest_QuantileTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Log1p_Winsorization\\feature_added_datasets\\phase2_Direct_Removal_X_train_Log1pWinsorization_with_features.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Log1p_Winsorization\\scaled_datasets\\phase2_Direct_Removal_X_train_Log1pWinsorization_StandardScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Log1p_Winsorization\\scaled_datasets\\phase2_Direct_Removal_X_train_Log1pWinsorization_MinMaxScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Log1p_Winsorization\\scaled_datasets\\phase2_Direct_Removal_X_train_Log1pWinsorization_RobustScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Log1p_Winsorization\\scaled_datasets\\phase2_Direct_Removal_X_train_Log1pWinsorization_PowerTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Log1p_Winsorization\\scaled_datasets\\phase2_Direct_Removal_X_train_Log1pWinsorization_QuantileTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Winsorization\\feature_added_datasets\\phase2_Direct_Removal_X_train_Winsorization_with_features.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Winsorization\\scaled_datasets\\phase2_Direct_Removal_X_train_Winsorization_StandardScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Winsorization\\scaled_datasets\\phase2_Direct_Removal_X_train_Winsorization_MinMaxScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Winsorization\\scaled_datasets\\phase2_Direct_Removal_X_train_Winsorization_RobustScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Winsorization\\scaled_datasets\\phase2_Direct_Removal_X_train_Winsorization_PowerTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Winsorization\\scaled_datasets\\phase2_Direct_Removal_X_train_Winsorization_QuantileTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Z-Score_Trimming\\feature_added_datasets\\phase2_Direct_Removal_X_train_ZScoreTrimming_with_features.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Z-Score_Trimming\\scaled_datasets\\phase2_Direct_Removal_X_train_ZScoreTrimming_StandardScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Z-Score_Trimming\\scaled_datasets\\phase2_Direct_Removal_X_train_ZScoreTrimming_MinMaxScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Z-Score_Trimming\\scaled_datasets\\phase2_Direct_Removal_X_train_ZScoreTrimming_RobustScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Z-Score_Trimming\\scaled_datasets\\phase2_Direct_Removal_X_train_ZScoreTrimming_PowerTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Z-Score_Trimming\\scaled_datasets\\phase2_Direct_Removal_X_train_ZScoreTrimming_QuantileTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Direct_Removal\\feature_added_datasets\\phase2_Instance_Weighting_X_train_DirectRemoval_with_features.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Direct_Removal\\scaled_datasets\\phase2_Instance_Weighting_X_train_DirectRemoval_StandardScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Direct_Removal\\scaled_datasets\\phase2_Instance_Weighting_X_train_DirectRemoval_MinMaxScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Direct_Removal\\scaled_datasets\\phase2_Instance_Weighting_X_train_DirectRemoval_RobustScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Direct_Removal\\scaled_datasets\\phase2_Instance_Weighting_X_train_DirectRemoval_PowerTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Direct_Removal\\scaled_datasets\\phase2_Instance_Weighting_X_train_DirectRemoval_QuantileTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Isolation_Forest\\feature_added_datasets\\phase2_Instance_Weighting_X_train_IsolationForest_with_features.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Isolation_Forest\\scaled_datasets\\phase2_Instance_Weighting_X_train_IsolationForest_StandardScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Isolation_Forest\\scaled_datasets\\phase2_Instance_Weighting_X_train_IsolationForest_MinMaxScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Isolation_Forest\\scaled_datasets\\phase2_Instance_Weighting_X_train_IsolationForest_RobustScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Isolation_Forest\\scaled_datasets\\phase2_Instance_Weighting_X_train_IsolationForest_PowerTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Isolation_Forest\\scaled_datasets\\phase2_Instance_Weighting_X_train_IsolationForest_QuantileTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Log1p_Winsorization\\feature_added_datasets\\phase2_Instance_Weighting_X_train_Log1pWinsorization_with_features.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Log1p_Winsorization\\scaled_datasets\\phase2_Instance_Weighting_X_train_Log1pWinsorization_StandardScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Log1p_Winsorization\\scaled_datasets\\phase2_Instance_Weighting_X_train_Log1pWinsorization_MinMaxScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Log1p_Winsorization\\scaled_datasets\\phase2_Instance_Weighting_X_train_Log1pWinsorization_RobustScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Log1p_Winsorization\\scaled_datasets\\phase2_Instance_Weighting_X_train_Log1pWinsorization_PowerTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Log1p_Winsorization\\scaled_datasets\\phase2_Instance_Weighting_X_train_Log1pWinsorization_QuantileTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Winsorization\\feature_added_datasets\\phase2_Instance_Weighting_X_train_Winsorization_with_features.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Winsorization\\scaled_datasets\\phase2_Instance_Weighting_X_train_Winsorization_StandardScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Winsorization\\scaled_datasets\\phase2_Instance_Weighting_X_train_Winsorization_MinMaxScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Winsorization\\scaled_datasets\\phase2_Instance_Weighting_X_train_Winsorization_RobustScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Winsorization\\scaled_datasets\\phase2_Instance_Weighting_X_train_Winsorization_PowerTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Winsorization\\scaled_datasets\\phase2_Instance_Weighting_X_train_Winsorization_QuantileTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Z-Score_Trimming\\feature_added_datasets\\phase2_Instance_Weighting_X_train_ZScoreTrimming_with_features.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Z-Score_Trimming\\scaled_datasets\\phase2_Instance_Weighting_X_train_ZScoreTrimming_StandardScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Z-Score_Trimming\\scaled_datasets\\phase2_Instance_Weighting_X_train_ZScoreTrimming_MinMaxScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Z-Score_Trimming\\scaled_datasets\\phase2_Instance_Weighting_X_train_ZScoreTrimming_RobustScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Z-Score_Trimming\\scaled_datasets\\phase2_Instance_Weighting_X_train_ZScoreTrimming_PowerTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Z-Score_Trimming\\scaled_datasets\\phase2_Instance_Weighting_X_train_ZScoreTrimming_QuantileTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Direct_Removal\\feature_added_datasets\\phase2_TrainTestAware_X_train_DirectRemoval_with_features.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Direct_Removal\\scaled_datasets\\phase2_TrainTestAware_X_train_DirectRemoval_StandardScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Direct_Removal\\scaled_datasets\\phase2_TrainTestAware_X_train_DirectRemoval_MinMaxScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Direct_Removal\\scaled_datasets\\phase2_TrainTestAware_X_train_DirectRemoval_RobustScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Direct_Removal\\scaled_datasets\\phase2_TrainTestAware_X_train_DirectRemoval_PowerTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Direct_Removal\\scaled_datasets\\phase2_TrainTestAware_X_train_DirectRemoval_QuantileTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Isolation_Forest\\feature_added_datasets\\phase2_TrainTestAware_X_train_IsolationForest_with_features.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Isolation_Forest\\scaled_datasets\\phase2_TrainTestAware_X_train_IsolationForest_StandardScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Isolation_Forest\\scaled_datasets\\phase2_TrainTestAware_X_train_IsolationForest_MinMaxScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Isolation_Forest\\scaled_datasets\\phase2_TrainTestAware_X_train_IsolationForest_RobustScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Isolation_Forest\\scaled_datasets\\phase2_TrainTestAware_X_train_IsolationForest_PowerTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Isolation_Forest\\scaled_datasets\\phase2_TrainTestAware_X_train_IsolationForest_QuantileTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Log1p_Winsorization\\feature_added_datasets\\phase2_TrainTestAware_X_train_Log1pWinsorization_with_features.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Log1p_Winsorization\\scaled_datasets\\phase2_TrainTestAware_X_train_Log1pWinsorization_StandardScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Log1p_Winsorization\\scaled_datasets\\phase2_TrainTestAware_X_train_Log1pWinsorization_MinMaxScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Log1p_Winsorization\\scaled_datasets\\phase2_TrainTestAware_X_train_Log1pWinsorization_RobustScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Log1p_Winsorization\\scaled_datasets\\phase2_TrainTestAware_X_train_Log1pWinsorization_PowerTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Log1p_Winsorization\\scaled_datasets\\phase2_TrainTestAware_X_train_Log1pWinsorization_QuantileTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Winsorization\\feature_added_datasets\\phase2_TrainTestAware_X_train_Winsorization_with_features.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Winsorization\\scaled_datasets\\phase2_TrainTestAware_X_train_Winsorization_StandardScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Winsorization\\scaled_datasets\\phase2_TrainTestAware_X_train_Winsorization_MinMaxScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Winsorization\\scaled_datasets\\phase2_TrainTestAware_X_train_Winsorization_RobustScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Winsorization\\scaled_datasets\\phase2_TrainTestAware_X_train_Winsorization_PowerTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Winsorization\\scaled_datasets\\phase2_TrainTestAware_X_train_Winsorization_QuantileTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Z-Score_Trimming\\feature_added_datasets\\phase2_TrainTestAware_X_train_ZScoreTrimming_with_features.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Z-Score_Trimming\\scaled_datasets\\phase2_TrainTestAware_X_train_ZScoreTrimming_StandardScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Z-Score_Trimming\\scaled_datasets\\phase2_TrainTestAware_X_train_ZScoreTrimming_MinMaxScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Z-Score_Trimming\\scaled_datasets\\phase2_TrainTestAware_X_train_ZScoreTrimming_RobustScaler.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Z-Score_Trimming\\scaled_datasets\\phase2_TrainTestAware_X_train_ZScoreTrimming_PowerTransformer.csv\n",
      "Created: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Z-Score_Trimming\\scaled_datasets\\phase2_TrainTestAware_X_train_ZScoreTrimming_QuantileTransformer.csv\n",
      "Feature engineering and scaling process complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer, QuantileTransformer\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Define the original columns to scale\n",
    "ORIGINAL_COLUMNS_TO_SCALE = [\n",
    "    'flow_time', 'header_size', 'packet_duration', 'overall_rate', \n",
    "    'src_rate', 'dst_rate', 'fin_packets', 'urg_packets', 'rst_packets', \n",
    "    'max_value', 'value_covariance'\n",
    "]\n",
    "\n",
    "# New features to be created and scaled\n",
    "NEW_FEATURES_TO_SCALE = [\n",
    "    'rate_ratio', 'syn_to_ack', 'rst_to_fin', 'avg_pkt_size',\n",
    "    'mean_interpkt', 'std_interpkt', 'p90_interpkt', 'burstiness',\n",
    "    'payload_entropy', 'value_range', 'flows_last_10s', 'unique_dsts_last_10s',\n",
    "    'hour_sin', 'hour_cos'\n",
    "]\n",
    "\n",
    "# Combined list of all columns to scale\n",
    "COLUMNS_TO_SCALE = ORIGINAL_COLUMNS_TO_SCALE + NEW_FEATURES_TO_SCALE\n",
    "\n",
    "# Binary features that should not be scaled\n",
    "BINARY_FEATURES = [\n",
    "    'handshake_complete', 'abrupt_reset', 'tcp_syn_ratio', 'udp_psh'\n",
    "]\n",
    "\n",
    "# Define the scalers to use\n",
    "SCALERS = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler(),\n",
    "    'PowerTransformer': PowerTransformer(method='yeo-johnson'),\n",
    "    'QuantileTransformer': QuantileTransformer(output_distribution='normal')\n",
    "}\n",
    "\n",
    "# Path to data directory containing deduplicated_datasets\n",
    "BASE_DIR = 'Data'\n",
    "DEDUP_DIR = os.path.join(BASE_DIR, 'deduplicated_datasets')\n",
    "\n",
    "# Main folders in deduplicated_datasets\n",
    "MAIN_FOLDERS = ['Direct_Removal', 'Instance_Weighting', 'Train_Test_Aware']\n",
    "\n",
    "def create_new_features(df):\n",
    "    \"\"\"\n",
    "    Create all the new features described in the feature summary\n",
    "    Adjusted to work with the available columns in the dataset\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Available columns:\n",
    "    # flow_time, header_size, packet_duration, overall_rate, src_rate, dst_rate,\n",
    "    # fin_packets, urg_packets, rst_packets, max_value, value_covariance,\n",
    "    # fin_flags, syn_flags, rst_flags, psh_flags, ack_flags,\n",
    "    # protocol_http, protocol_https, protocol_tcp, protocol_udp, protocol_icmp\n",
    "    \n",
    "    # Small epsilon value to avoid division by zero\n",
    "    epsilon = 1e-10\n",
    "    \n",
    "    # 1. Continuous features that need scaling\n",
    "    \n",
    "    # rate_ratio: (src_rate + ε) / (dst_rate + ε)\n",
    "    df_copy['rate_ratio'] = (df_copy['src_rate'] + epsilon) / (df_copy['dst_rate'] + epsilon)\n",
    "    \n",
    "    # syn_to_ack: (syn_flags + 1) / (ack_flags + 1) - using flags instead of packet counts\n",
    "    df_copy['syn_to_ack'] = (df_copy['syn_flags'] + 1) / (df_copy['ack_flags'] + 1)\n",
    "    \n",
    "    # rst_to_fin: (rst_packets + 1) / (fin_packets + 1)\n",
    "    df_copy['rst_to_fin'] = (df_copy['rst_packets'] + 1) / (df_copy['fin_packets'] + 1)\n",
    "    \n",
    "    # Estimate total_packets from available metrics\n",
    "    # This is an approximation - adjust the formula based on your domain knowledge\n",
    "    df_copy['total_packets_est'] = (df_copy['fin_packets'] + df_copy['urg_packets'] + \n",
    "                                   df_copy['rst_packets'] + 2)  # Adding 2 as base minimum\n",
    "    \n",
    "    # avg_pkt_size: approximate using available metrics\n",
    "    df_copy['avg_pkt_size'] = (df_copy['overall_rate'] * df_copy['flow_time']) / df_copy['total_packets_est']\n",
    "    \n",
    "    # Inter-packet features using the estimated total_packets\n",
    "    avg_gap = df_copy['flow_time'] / df_copy['total_packets_est'].clip(lower=2)\n",
    "    df_copy['mean_interpkt'] = avg_gap\n",
    "    df_copy['std_interpkt'] = avg_gap * (df_copy['value_covariance'].clip(lower=0.1))\n",
    "    df_copy['p90_interpkt'] = avg_gap * 1.5  # Rough estimate for 90th percentile\n",
    "    \n",
    "    # Burstiness: using available metrics to approximate\n",
    "    df_copy['burstiness'] = 2.0 + df_copy['value_covariance']  # Rough approximation\n",
    "    \n",
    "    # payload_entropy (approximated using available statistics)\n",
    "    df_copy['payload_entropy'] = df_copy['value_covariance'].clip(lower=0) + 1\n",
    "    \n",
    "    # value_range: Since no min_value, approximate from max_value\n",
    "    df_copy['value_range'] = df_copy['max_value'] * 0.8\n",
    "    \n",
    "    # flows_last_10s and unique_dsts_last_10s - use approximations\n",
    "    df_copy['flows_last_10s'] = (df_copy['src_rate'] * 10).clip(lower=1)\n",
    "    df_copy['unique_dsts_last_10s'] = (df_copy['flows_last_10s'] * 0.7).clip(lower=1)\n",
    "    \n",
    "    # Time-based cyclical features - use constants since no time data\n",
    "    # You could consider deriving time from other context if available\n",
    "    df_copy['hour_sin'] = 0  # Default placeholder\n",
    "    df_copy['hour_cos'] = 1  # Default placeholder\n",
    "    \n",
    "    # 2. Binary features (no scaling needed)\n",
    "    \n",
    "    # handshake_complete: using flags instead of packet counts\n",
    "    df_copy['handshake_complete'] = ((df_copy['syn_flags'] > 0) & \n",
    "                                    (df_copy['ack_flags'] > 0)).astype(int)\n",
    "    \n",
    "    # abrupt_reset: 1 if rst_flags=1 AND fin_flags=0, else 0\n",
    "    df_copy['abrupt_reset'] = ((df_copy['rst_flags'] > 0) & \n",
    "                              (df_copy['fin_flags'] == 0)).astype(int)\n",
    "    \n",
    "    # tcp_syn_ratio: syn_flags * protocol_tcp\n",
    "    df_copy['tcp_syn_ratio'] = df_copy['syn_flags'] * df_copy['protocol_tcp']\n",
    "    \n",
    "    # udp_psh: psh_flags * protocol_udp\n",
    "    df_copy['udp_psh'] = df_copy['psh_flags'] * df_copy['protocol_udp']\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def apply_scaling(dataframe, scaler, columns_to_scale):\n",
    "    \"\"\"Apply scaling to specified columns of the dataframe\"\"\"\n",
    "    df_copy = dataframe.copy()\n",
    "    \n",
    "    # Check which columns actually exist in the dataframe\n",
    "    existing_columns = [col for col in columns_to_scale if col in df_copy.columns]\n",
    "    \n",
    "    if existing_columns:\n",
    "        # Extract the columns to scale\n",
    "        data_to_scale = df_copy[existing_columns].values\n",
    "        \n",
    "        # Fit and transform the data\n",
    "        scaled_data = scaler.fit_transform(data_to_scale)\n",
    "        \n",
    "        # Replace the original columns with scaled data\n",
    "        df_copy[existing_columns] = scaled_data\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def process_files():\n",
    "    \"\"\"Process files in the specified directory structure\"\"\"\n",
    "    for main_folder in MAIN_FOLDERS:\n",
    "        main_path = os.path.join(DEDUP_DIR, main_folder)\n",
    "        outlier_path = os.path.join(main_path, 'outlier_handled_datasets')\n",
    "        \n",
    "        # Skip if path doesn't exist\n",
    "        if not os.path.exists(outlier_path):\n",
    "            print(f\"Path not found: {outlier_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Loop through each outlier handling method folder\n",
    "        for outlier_method in os.listdir(outlier_path):\n",
    "            method_path = os.path.join(outlier_path, outlier_method)\n",
    "            \n",
    "            # Skip if not a directory\n",
    "            if not os.path.isdir(method_path):\n",
    "                continue\n",
    "            \n",
    "            # Create the scaled_datasets directory if it doesn't exist\n",
    "            scaled_path = os.path.join(method_path, 'scaled_datasets')\n",
    "            os.makedirs(scaled_path, exist_ok=True)\n",
    "            \n",
    "            # Create an intermediate directory for feature-added datasets\n",
    "            feature_added_path = os.path.join(method_path, 'feature_added_datasets')\n",
    "            os.makedirs(feature_added_path, exist_ok=True)\n",
    "            \n",
    "            # Find X_train files\n",
    "            for file in os.listdir(method_path):\n",
    "                if file.endswith('.csv') and 'X_' in file:\n",
    "                    file_path = os.path.join(method_path, file)\n",
    "                    \n",
    "                    try:\n",
    "                        # Read the CSV file\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        \n",
    "                        # Get the base filename without extension\n",
    "                        base_name = os.path.splitext(file)[0]\n",
    "                        \n",
    "                        # First, add the new features to the dataframe\n",
    "                        df_with_features = create_new_features(df)\n",
    "                        \n",
    "                        # Save the intermediate dataframe with added features\n",
    "                        feature_added_file = f\"{base_name}_with_features.csv\"\n",
    "                        feature_added_path_file = os.path.join(feature_added_path, feature_added_file)\n",
    "                        df_with_features.to_csv(feature_added_path_file, index=False)\n",
    "                        print(f\"Created: {feature_added_path_file}\")\n",
    "                        \n",
    "                        # Apply each scaler and save the result\n",
    "                        for scaler_name, scaler in SCALERS.items():\n",
    "                            # Apply scaling to the dataframe (only scale continuous features)\n",
    "                            scaled_df = apply_scaling(df_with_features, scaler, COLUMNS_TO_SCALE)\n",
    "                            \n",
    "                            # Create output filename\n",
    "                            output_file = f\"{base_name}_{scaler_name}.csv\"\n",
    "                            output_path = os.path.join(scaled_path, output_file)\n",
    "                            \n",
    "                            # Save the scaled dataframe\n",
    "                            scaled_df.to_csv(output_path, index=False)\n",
    "                            \n",
    "                            print(f\"Created: {output_path}\")\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting to process files for feature engineering and scaling...\")\n",
    "    process_files()\n",
    "    print(\"Feature engineering and scaling process complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac6ade62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to recreate scaled_datasets directories...\n",
      "Created directory: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Direct_Removal\\scaled_datasets\n",
      "Created directory: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Isolation_Forest\\scaled_datasets\n",
      "Created directory: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Log1p_Winsorization\\scaled_datasets\n",
      "Created directory: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Winsorization\\scaled_datasets\n",
      "Created directory: Data\\deduplicated_datasets\\Direct_Removal\\outlier_handled_datasets\\Z-Score_Trimming\\scaled_datasets\n",
      "Created directory: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Direct_Removal\\scaled_datasets\n",
      "Created directory: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Isolation_Forest\\scaled_datasets\n",
      "Created directory: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Log1p_Winsorization\\scaled_datasets\n",
      "Created directory: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Winsorization\\scaled_datasets\n",
      "Created directory: Data\\deduplicated_datasets\\Instance_Weighting\\outlier_handled_datasets\\Z-Score_Trimming\\scaled_datasets\n",
      "Created directory: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Direct_Removal\\scaled_datasets\n",
      "Created directory: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Isolation_Forest\\scaled_datasets\n",
      "Created directory: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Log1p_Winsorization\\scaled_datasets\n",
      "Created directory: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Winsorization\\scaled_datasets\n",
      "Created directory: Data\\deduplicated_datasets\\Train_Test_Aware\\outlier_handled_datasets\\Z-Score_Trimming\\scaled_datasets\n",
      "Recreation process complete! Total directories created: 15\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to data directory containing deduplicated_datasets\n",
    "BASE_DIR = 'Data'\n",
    "DEDUP_DIR = os.path.join(BASE_DIR, 'deduplicated_datasets')\n",
    "\n",
    "# Main folders in deduplicated_datasets\n",
    "MAIN_FOLDERS = ['Direct_Removal', 'Instance_Weighting', 'Train_Test_Aware']\n",
    "\n",
    "def recreate_scaled_directories():\n",
    "    \"\"\"Recreate the scaled_datasets directories in the appropriate locations\"\"\"\n",
    "    directories_created = 0\n",
    "    \n",
    "    print(\"Starting to recreate scaled_datasets directories...\")\n",
    "    \n",
    "    for main_folder in MAIN_FOLDERS:\n",
    "        main_path = os.path.join(DEDUP_DIR, main_folder)\n",
    "        outlier_path = os.path.join(main_path, 'outlier_handled_datasets')\n",
    "        \n",
    "        # Skip if path doesn't exist\n",
    "        if not os.path.exists(outlier_path):\n",
    "            print(f\"Path not found: {outlier_path}\")\n",
    "            continue\n",
    "            \n",
    "        # Loop through each outlier handling method folder\n",
    "        for outlier_method in os.listdir(outlier_path):\n",
    "            method_path = os.path.join(outlier_path, outlier_method)\n",
    "            \n",
    "            # Skip if not a directory\n",
    "            if not os.path.isdir(method_path):\n",
    "                continue\n",
    "                \n",
    "            # Create the scaled_datasets directory\n",
    "            scaled_path = os.path.join(method_path, 'scaled_datasets')\n",
    "            \n",
    "            if not os.path.exists(scaled_path):\n",
    "                try:\n",
    "                    os.makedirs(scaled_path)\n",
    "                    print(f\"Created directory: {scaled_path}\")\n",
    "                    directories_created += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error creating directory {scaled_path}: {e}\")\n",
    "            else:\n",
    "                print(f\"Directory already exists: {scaled_path}\")\n",
    "    \n",
    "    print(f\"Recreation process complete! Total directories created: {directories_created}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    recreate_scaled_directories()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cc3186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "import pdfkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d24bda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas is already installed.\n",
      "numpy is already installed.\n",
      "Installing scikit-learn...\n",
      "xgboost is already installed.\n",
      "lightgbm is already installed.\n",
      "catboost is already installed.\n",
      "Installing imbalanced-learn...\n",
      "Loading raw dataset...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 256. KiB for an array with shape (32768,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 287\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading raw dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    286\u001b[0m raw_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMachine Learning\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPhase 2\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mphase2_students_before_cleaning.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 287\u001b[0m raw_df \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_inspect_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_data_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# Apply data cleaning and preprocessing\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCleaning data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 46\u001b[0m, in \u001b[0;36mload_and_inspect_data\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_inspect_data\u001b[39m(file_path):\n\u001b[0;32m     45\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load the raw dataset and print basic information\"\"\"\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset loaded successfully. Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDataset Info:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:921\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1066\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1120\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1222\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1833\u001b[0m, in \u001b[0;36mpandas._libs.parsers._try_int64\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 256. KiB for an array with shape (32768,) and data type int64"
     ]
    }
   ],
   "source": [
    "# First, install required packages if they're not already installed\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    print(f\"Installing {package}...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "required_packages = ['pandas', 'numpy', 'scikit-learn', 'xgboost', 'lightgbm', 'catboost', 'imbalanced-learn']\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        install_package(package)\n",
    "\n",
    "# Now import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.feature_selection import SelectFromModel, RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ============= NEW DATA CLEANING AND PREPROCESSING FUNCTIONS =============\n",
    "\n",
    "def load_and_inspect_data(file_path):\n",
    "    \"\"\"Load the raw dataset and print basic information\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "    print(\"\\nDataset Info:\")\n",
    "    print(df.info())\n",
    "    print(\"\\nMissing values per column:\")\n",
    "    print(df.isnull().sum())\n",
    "    print(\"\\nData sample:\")\n",
    "    print(df.head())\n",
    "    return df\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"Perform basic data cleaning operations\"\"\"\n",
    "    # Make a copy of the original dataframe\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    # 1. Handle duplicates\n",
    "    original_rows = df_cleaned.shape[0]\n",
    "    df_cleaned = df_cleaned.drop_duplicates()\n",
    "    print(f\"Removed {original_rows - df_cleaned.shape[0]} duplicate rows\")\n",
    "    \n",
    "    # 2. Convert data types if needed\n",
    "    # Convert object columns that should be numeric\n",
    "    for col in df_cleaned.columns:\n",
    "        if df_cleaned[col].dtype == 'object':\n",
    "            try:\n",
    "                # Try to convert to numeric\n",
    "                df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')\n",
    "                print(f\"Converted {col} to numeric\")\n",
    "            except:\n",
    "                print(f\"Keeping {col} as object type\")\n",
    "    \n",
    "    # 3. Handle missing values\n",
    "    # First, identify columns with missing values\n",
    "    missing_cols = df_cleaned.columns[df_cleaned.isnull().any()].tolist()\n",
    "    print(f\"Columns with missing values: {missing_cols}\")\n",
    "    \n",
    "    # For numeric columns, impute with median - FIXED to handle one column at a time\n",
    "    numeric_cols = df_cleaned.select_dtypes(include=['float64', 'int64']).columns\n",
    "    missing_numeric_cols = [col for col in missing_cols if col in numeric_cols]\n",
    "    \n",
    "    if missing_numeric_cols:\n",
    "        print(f\"Imputing missing values in numeric columns with median\")\n",
    "        for col in missing_numeric_cols:\n",
    "            if df_cleaned[col].isnull().any():\n",
    "                # Create a 2D array for the imputer by reshaping\n",
    "                col_data = df_cleaned[col].values.reshape(-1, 1)\n",
    "                imputer = SimpleImputer(strategy='median')\n",
    "                df_cleaned[col] = imputer.fit_transform(col_data).ravel()\n",
    "    \n",
    "    # For categorical columns, impute with most frequent value - FIXED to handle one column at a time\n",
    "    categorical_cols = df_cleaned.select_dtypes(include=['object']).columns\n",
    "    missing_categorical_cols = [col for col in missing_cols if col in categorical_cols]\n",
    "    \n",
    "    if missing_categorical_cols:\n",
    "        print(f\"Imputing missing values in categorical columns with most frequent value\")\n",
    "        for col in missing_categorical_cols:\n",
    "            if df_cleaned[col].isnull().any():\n",
    "                # Create a 2D array for the imputer\n",
    "                col_data = df_cleaned[col].values.reshape(-1, 1)\n",
    "                cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "                df_cleaned[col] = cat_imputer.fit_transform(col_data).ravel()\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "def handle_outliers(df, method='winsorize'):\n",
    "    \"\"\"Handle outliers in numeric columns\"\"\"\n",
    "    df_processed = df.copy()\n",
    "    numeric_cols = df_processed.select_dtypes(include=['float64', 'int64']).columns\n",
    "    \n",
    "    if method == 'winsorize':\n",
    "        for col in numeric_cols:\n",
    "            # Skip target column if present\n",
    "            if col.lower() in ['target', 'label', 'class', 'y']:\n",
    "                continue\n",
    "                \n",
    "            # Calculate Q1, Q3 and IQR\n",
    "            Q1 = df_processed[col].quantile(0.25)\n",
    "            Q3 = df_processed[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            # Define bounds for winsorization\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            # Winsorize (clip) values outside the bounds\n",
    "            df_processed[col] = df_processed[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "        \n",
    "        print(\"Applied winsorization to handle outliers\")\n",
    "    \n",
    "    elif method == 'remove':\n",
    "        for col in numeric_cols:\n",
    "            # Skip target column if present\n",
    "            if col.lower() in ['target', 'label', 'class', 'y']:\n",
    "                continue\n",
    "                \n",
    "            # Calculate Q1, Q3 and IQR\n",
    "            Q1 = df_processed[col].quantile(0.25)\n",
    "            Q3 = df_processed[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            # Define bounds\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            # Flag outliers\n",
    "            outliers = (df_processed[col] < lower_bound) | (df_processed[col] > upper_bound)\n",
    "            \n",
    "            # Replace outliers with NaN\n",
    "            df_processed.loc[outliers, col] = np.nan\n",
    "            \n",
    "        # Re-impute after outlier removal\n",
    "        for col in numeric_cols:\n",
    "            if df_processed[col].isnull().any():\n",
    "                col_data = df_processed[col].values.reshape(-1, 1)\n",
    "                imputer = SimpleImputer(strategy='median')\n",
    "                df_processed[col] = imputer.fit_transform(col_data).ravel()\n",
    "        \n",
    "        print(\"Identified and handled outliers by replacing with NaN and then imputing\")\n",
    "    \n",
    "    return df_processed\n",
    "    \n",
    "def encode_categorical_features(df):\n",
    "    \"\"\"Encode categorical features using one-hot encoding\"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    # Identify categorical columns (excluding the target column)\n",
    "    categorical_cols = df_encoded.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Apply one-hot encoding to categorical columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"One-hot encoding {len(categorical_cols)} categorical features\")\n",
    "        df_encoded = pd.get_dummies(df_encoded, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "def scale_features(df, scaler_type='robust'):\n",
    "    \"\"\"Scale numeric features using the specified scaler\"\"\"\n",
    "    # Make a copy\n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    # Identify numeric columns (excluding the target column)\n",
    "    numeric_cols = df_scaled.select_dtypes(include=['float64', 'int64']).columns\n",
    "    # Remove potential target columns\n",
    "    numeric_cols = [col for col in numeric_cols if col.lower() not in ['target', 'label', 'class', 'y']]\n",
    "    \n",
    "    if len(numeric_cols) > 0:\n",
    "        # Apply scaling on a column-by-column basis to avoid shape issues\n",
    "        if scaler_type == 'standard':\n",
    "            for col in numeric_cols:\n",
    "                scaler = StandardScaler()\n",
    "                df_scaled[col] = scaler.fit_transform(df_scaled[col].values.reshape(-1, 1)).ravel()\n",
    "        elif scaler_type == 'robust':\n",
    "            for col in numeric_cols:\n",
    "                scaler = RobustScaler()\n",
    "                df_scaled[col] = scaler.fit_transform(df_scaled[col].values.reshape(-1, 1)).ravel()\n",
    "        elif scaler_type == 'minmax':\n",
    "            for col in numeric_cols:\n",
    "                scaler = MinMaxScaler()\n",
    "                df_scaled[col] = scaler.fit_transform(df_scaled[col].values.reshape(-1, 1)).ravel()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scaler type: {scaler_type}\")\n",
    "        \n",
    "        print(f\"Applied {scaler_type} scaling to {len(numeric_cols)} numeric features\")\n",
    "    \n",
    "    return df_scaled\n",
    "\n",
    "# ============= EXISTING HELPER FUNCTIONS UPDATED =============\n",
    "\n",
    "def log_transform(X):\n",
    "    \"\"\"Apply log transformation to numeric features\"\"\"\n",
    "    numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "    # Skip potential target columns\n",
    "    numeric_cols = [col for col in numeric_cols if col.lower() not in ['target', 'label', 'class', 'y']]\n",
    "    \n",
    "    # Add a small constant to avoid log(0)\n",
    "    X_transformed = X.copy()\n",
    "    for col in numeric_cols:\n",
    "        # Skip columns with negative values or zeros\n",
    "        if (X[col] <= 0).any():\n",
    "            # For columns with zero or negative values, use log1p(x + |min| + 1) if needed\n",
    "            if (X[col] < 0).any():\n",
    "                min_val = abs(X[col].min()) + 1\n",
    "                X_transformed[col] = np.log1p(X[col] + min_val)\n",
    "            else:\n",
    "                # For columns with zeros but no negative values\n",
    "                X_transformed[col] = np.log1p(X[col])\n",
    "        else:\n",
    "            # Standard log for strictly positive columns\n",
    "            X_transformed[col] = np.log(X[col])\n",
    "    \n",
    "    return X_transformed\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"Evaluate model and return metrics\"\"\"\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # For binary classification ensure we have probability estimates for AUC\n",
    "        try:\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        except:\n",
    "            auc = None\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        results = {\n",
    "            'Model': model_name,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1,\n",
    "            'ROC AUC': auc,\n",
    "            'Inference Time (s)': inference_time,\n",
    "            'Error': None\n",
    "        }\n",
    "    except Exception as e:\n",
    "        results = {\n",
    "            'Model': model_name,\n",
    "            'Accuracy': None,\n",
    "            'Precision': None,\n",
    "            'Recall': None,\n",
    "            'F1 Score': None,\n",
    "            'ROC AUC': None,\n",
    "            'Inference Time (s)': None,\n",
    "            'Error': str(e)\n",
    "        }\n",
    "        print(f\"Error evaluating {model_name}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ============= MAIN EXECUTION CODE =============\n",
    "\n",
    "# Load raw dataset\n",
    "print(\"Loading raw dataset...\")\n",
    "raw_data_path = r\"C:\\Machine Learning\\Phase 2\\Data\\phase2_students_before_cleaning.csv\"\n",
    "raw_df = load_and_inspect_data(raw_data_path)\n",
    "\n",
    "# Apply data cleaning and preprocessing\n",
    "print(\"\\nCleaning data...\")\n",
    "df_cleaned = clean_data(raw_df)\n",
    "\n",
    "print(\"\\nHandling outliers...\")\n",
    "df_no_outliers = handle_outliers(df_cleaned, method='winsorize')\n",
    "\n",
    "print(\"\\nEncoding categorical features...\")\n",
    "df_encoded = encode_categorical_features(df_no_outliers)\n",
    "\n",
    "print(\"\\nIdentifying target column...\")\n",
    "# Assuming the target column is called 'target', 'label', 'class', or 'y'\n",
    "# Let's try to identify it automatically\n",
    "target_candidates = [col for col in df_encoded.columns if col.lower() in ['target', 'label', 'class', 'y']]\n",
    "\n",
    "if target_candidates:\n",
    "    target_column = target_candidates[0]\n",
    "    print(f\"Target column identified: {target_column}\")\n",
    "else:\n",
    "    # If no standard name is found, ask the user to provide the target column name\n",
    "    print(\"No standard target column name found in the dataset.\")\n",
    "    print(\"Available columns:\", df_encoded.columns.tolist())\n",
    "    print(\"Assuming the last column is the target variable\")\n",
    "    target_column = df_encoded.columns[-1]\n",
    "    print(f\"Using '{target_column}' as the target variable\")\n",
    "\n",
    "# Split features and target\n",
    "X = df_encoded.drop(columns=[target_column])\n",
    "y = df_encoded[target_column]\n",
    "\n",
    "# Ensure the target is numeric\n",
    "try:\n",
    "    y = y.astype(int)\n",
    "    print(f\"Target variable converted to integer type\")\n",
    "except:\n",
    "    print(f\"Warning: Could not convert target to integer type. Using as-is.\")\n",
    "\n",
    "# Check if target is binary\n",
    "unique_classes = y.nunique()\n",
    "if unique_classes > 2:\n",
    "    print(f\"Target has {unique_classes} classes. Treating as a multi-class problem.\")\n",
    "else:\n",
    "    print(\"Target is binary. Treating as a binary classification problem.\")\n",
    "\n",
    "# Split into train and test sets\n",
    "print(\"\\nSplitting data into train and test sets...\")\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y if y.nunique() > 1 else None\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"Warning: Could not stratify split due to: {e}\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "try:\n",
    "    print(f\"Class distribution in y_train: {np.bincount(y_train.astype(int))}\")\n",
    "    print(f\"Class distribution in y_test: {np.bincount(y_test.astype(int))}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Warning: Could not print class distribution: {e}\")\n",
    "    print(f\"y_train unique values: {y_train.unique()}\")\n",
    "    print(f\"y_test unique values: {y_test.unique()}\")\n",
    "\n",
    "# Scale features\n",
    "print(\"\\nScaling features...\")\n",
    "X_train = scale_features(X_train, scaler_type='robust')\n",
    "X_test = scale_features(X_test, scaler_type='robust')\n",
    "\n",
    "# Continue with the existing ML pipeline from the original code\n",
    "# Define imbalance handling techniques\n",
    "imbalance_techniques = {\n",
    "    'Original': None,\n",
    "    'SMOTE': SMOTE(random_state=RANDOM_STATE),\n",
    "    'ADASYN': ADASYN(random_state=RANDOM_STATE),\n",
    "    'RandomOverSampler': RandomOverSampler(random_state=RANDOM_STATE),\n",
    "    'RandomUnderSampler': RandomUnderSampler(random_state=RANDOM_STATE),\n",
    "    'TomekLinks': TomekLinks(),\n",
    "    'NearMiss': NearMiss(),\n",
    "    'SMOTETomek': SMOTETomek(random_state=RANDOM_STATE),\n",
    "    'SMOTEENN': SMOTEENN(random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "# Define feature selection techniques\n",
    "feature_selection_techniques = {\n",
    "    'NoSelection': None,\n",
    "    'SelectFromModel_RF': SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)),\n",
    "    # 'RFE_RF': RFE(RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE), n_features_to_select=X_train.shape[1] // 2)\n",
    "}\n",
    "\n",
    "# Define model creation functions instead of instances\n",
    "def get_model(model_name, class_ratio=None):\n",
    "    \"\"\"Get a fresh model instance with appropriate parameters for class imbalance\"\"\"\n",
    "    if model_name == 'XGBoost':\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=100, \n",
    "            learning_rate=0.1, \n",
    "            max_depth=6, \n",
    "            random_state=RANDOM_STATE,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "        if class_ratio is not None:\n",
    "            model.set_params(scale_pos_weight=class_ratio)\n",
    "    elif model_name == 'LightGBM':\n",
    "        model = LGBMClassifier(\n",
    "            n_estimators=100, \n",
    "            learning_rate=0.1, \n",
    "            max_depth=6, \n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "        if class_ratio is not None:\n",
    "            model.set_params(is_unbalance=True)\n",
    "    elif model_name == 'CatBoost':\n",
    "        model = CatBoostClassifier(\n",
    "            iterations=100, \n",
    "            learning_rate=0.1, \n",
    "            depth=6, \n",
    "            random_seed=RANDOM_STATE,\n",
    "            verbose=0\n",
    "        )\n",
    "        if class_ratio is not None:\n",
    "            model.set_params(auto_class_weights='Balanced')\n",
    "    elif model_name == 'HIST-G-Boosting':\n",
    "        model = HistGradientBoostingClassifier(\n",
    "            max_iter=100, \n",
    "            learning_rate=0.1, \n",
    "            max_depth=6, \n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "    elif model_name == 'ExtraTrees':\n",
    "        model = ExtraTreesClassifier(\n",
    "            n_estimators=100, \n",
    "            max_depth=6, \n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "    elif model_name == 'BalancedRandomForest':\n",
    "        model = BalancedRandomForestClassifier(\n",
    "            n_estimators=100, \n",
    "            max_depth=6, \n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "    elif model_name == 'RandomForest':\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=100, \n",
    "            max_depth=6, \n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "    elif model_name == 'DecisionTree':\n",
    "        model = DecisionTreeClassifier(\n",
    "            max_depth=6, \n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# List of model names\n",
    "model_names = [\n",
    "    'XGBoost', 'LightGBM', 'CatBoost', 'HIST-G-Boosting', \n",
    "    'ExtraTrees', 'BalancedRandomForest', 'RandomForest', 'DecisionTree'\n",
    "]\n",
    "\n",
    "# Main execution loop for model training and evaluation\n",
    "all_results = []\n",
    "\n",
    "for imbalance_name, imbalance_technique in imbalance_techniques.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Applying imbalance technique: {imbalance_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Apply imbalance handling (if needed)\n",
    "    try:\n",
    "        if imbalance_technique:\n",
    "            X_train_resampled, y_train_resampled = imbalance_technique.fit_resample(X_train, y_train)\n",
    "            try:\n",
    "                print(f\"After resampling - X_train shape: {X_train_resampled.shape}, Class distribution: {np.bincount(y_train_resampled.astype(int))}\")\n",
    "            except ValueError:\n",
    "                print(f\"After resampling - X_train shape: {X_train_resampled.shape}\")\n",
    "        else:\n",
    "            X_train_resampled, y_train_resampled = X_train.copy(), y_train.copy()\n",
    "        \n",
    "        # Apply Log Transformation\n",
    "        X_train_transformed = log_transform(X_train_resampled)\n",
    "        X_test_transformed = log_transform(X_test.copy())\n",
    "        print(\"Applied Log Transformation\")\n",
    "        \n",
    "        # Shuffle if required - FIX: use test_size=0.0001 instead of 0\n",
    "        # This keeps almost all data in the training set but satisfies scikit-learn's validation\n",
    "        X_train_shuffled, _, y_train_shuffled, _ = train_test_split(\n",
    "            X_train_transformed, y_train_resampled, test_size=0.0001, random_state=RANDOM_STATE\n",
    "        )\n",
    "        print(\"Applied shuffling\")\n",
    "        \n",
    "        # Calculate class ratio for imbalanced data handling if needed\n",
    "        class_ratio = None\n",
    "        try:\n",
    "            if len(np.unique(y_train_shuffled)) > 1:\n",
    "                class_counts = np.bincount(y_train_shuffled.astype(int))\n",
    "                if len(class_counts) > 1 and class_counts[0] != class_counts[1]:\n",
    "                    class_ratio = class_counts[0] / class_counts[1]\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not calculate class ratio: {e}\")\n",
    "        \n",
    "        for fs_name, fs_technique in feature_selection_techniques.items():\n",
    "            print(f\"\\n{'-'*30}\")\n",
    "            print(f\"Feature selection: {fs_name}\")\n",
    "            print(f\"{'-'*30}\")\n",
    "            \n",
    "            try:\n",
    "                # Apply feature selection\n",
    "                if fs_technique:\n",
    "                    fs_technique.fit(X_train_shuffled, y_train_shuffled)\n",
    "                    selected_features = X_train_shuffled.columns[fs_technique.get_support()]\n",
    "                    X_train_selected = X_train_shuffled[selected_features]\n",
    "                    X_test_selected = X_test_transformed[selected_features]\n",
    "                    print(f\"Selected {len(selected_features)} features\")\n",
    "                else:\n",
    "                    X_train_selected = X_train_shuffled\n",
    "                    X_test_selected = X_test_transformed\n",
    "                \n",
    "                # Train and evaluate models\n",
    "                for model_name in model_names:\n",
    "                    print(f\"Training {model_name}...\")\n",
    "                    \n",
    "                    try:\n",
    "                        # Get a fresh model instance with appropriate parameters\n",
    "                        model = get_model(model_name, class_ratio)\n",
    "                        \n",
    "                        # Train the model\n",
    "                        start_time = time.time()\n",
    "                        model.fit(X_train_selected, y_train_shuffled)\n",
    "                        training_time = time.time() - start_time\n",
    "                        \n",
    "                        # Evaluate\n",
    "                        results = evaluate_model(model, X_test_selected, y_test, model_name)\n",
    "                        \n",
    "                        # Add additional info\n",
    "                        results.update({\n",
    "                            'Imbalance Technique': imbalance_name,\n",
    "                            'Feature Selection': fs_name,\n",
    "                            'Training Time (s)': training_time,\n",
    "                            'Number of Features': X_train_selected.shape[1]\n",
    "                        })\n",
    "                        \n",
    "                        all_results.append(results)\n",
    "                        print(f\"Completed {model_name}. Training time: {training_time:.2f}s, Accuracy: {results['Accuracy']}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error training {model_name}: {e}\")\n",
    "                        all_results.append({\n",
    "                            'Model': model_name,\n",
    "                            'Imbalance Technique': imbalance_name,\n",
    "                            'Feature Selection': fs_name,\n",
    "                            'Accuracy': None,\n",
    "                            'Precision': None,\n",
    "                            'Recall': None,\n",
    "                            'F1 Score': None,\n",
    "                            'ROC AUC': None,\n",
    "                            'Training Time (s)': None,\n",
    "                            'Inference Time (s)': None,\n",
    "                            'Number of Features': X_train_selected.shape[1] if 'X_train_selected' in locals() else None,\n",
    "                            'Error': str(e)\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(f\"Error in feature selection {fs_name}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in imbalance technique {imbalance_name}: {e}\")\n",
    "\n",
    "# Convert to DataFrame and display results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(\"\\nComplete Results:\")\n",
    "print(results_df)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('ml_pipeline_results_with_cleaning.csv', index=False)\n",
    "print(\"\\nResults saved to 'ml_pipeline_results_with_cleaning.csv'\")\n",
    "\n",
    "# Find best model based on F1 score if available\n",
    "try:\n",
    "    # Filter out rows with errors or None F1 scores\n",
    "    valid_results = results_df.dropna(subset=['F1 Score'])\n",
    "    \n",
    "    if not valid_results.empty:\n",
    "        best_model_idx = valid_results['F1 Score'].idxmax()\n",
    "        best_model = results_df.iloc[best_model_idx]\n",
    "        print(\"\\nBest Model Configuration:\")\n",
    "        print(f\"Model: {best_model['Model']}\")\n",
    "        print(f\"Imbalance Technique: {best_model['Imbalance Technique']}\")\n",
    "        print(f\"Feature Selection: {best_model['Feature Selection']}\")\n",
    "        print(f\"F1 Score: {best_model['F1 Score']:.4f}\")\n",
    "        print(f\"Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "        print(f\"ROC AUC: {best_model['ROC AUC']:.4f}\" if best_model['ROC AUC'] else \"ROC AUC: N/A\")\n",
    "    else:\n",
    "        print(\"\\nNo valid models with F1 scores found.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError finding best model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c1220f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:11: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17760\\1423733647.py:11: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  data_path = \"C:\\Machine Learning\\Phase 2\\Data\\phase2_students_before_cleaning.csv\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2417\n",
      "[LightGBM] [Info] Number of data points in the train set: 842396, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n",
      "[LightGBM] [Info] Start training from score -1.791759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1/5...\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttrain's multi_logloss: 0.337805\tvalid's multi_logloss: 0.344537\n",
      "[200]\ttrain's multi_logloss: 0.292102\tvalid's multi_logloss: 0.326038\n",
      "[300]\ttrain's multi_logloss: 0.262308\tvalid's multi_logloss: 0.318283\n",
      "[400]\ttrain's multi_logloss: 0.239991\tvalid's multi_logloss: 0.313716\n",
      "[500]\ttrain's multi_logloss: 0.221219\tvalid's multi_logloss: 0.310867\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttrain's multi_logloss: 0.221219\tvalid's multi_logloss: 0.310867\n",
      "Training fold 2/5...\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttrain's multi_logloss: 0.337398\tvalid's multi_logloss: 0.347813\n",
      "[200]\ttrain's multi_logloss: 0.292207\tvalid's multi_logloss: 0.329002\n",
      "[300]\ttrain's multi_logloss: 0.262185\tvalid's multi_logloss: 0.320624\n",
      "[400]\ttrain's multi_logloss: 0.239823\tvalid's multi_logloss: 0.316033\n",
      "[500]\ttrain's multi_logloss: 0.221727\tvalid's multi_logloss: 0.313014\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttrain's multi_logloss: 0.221727\tvalid's multi_logloss: 0.313014\n",
      "Training fold 3/5...\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttrain's multi_logloss: 0.338896\tvalid's multi_logloss: 0.345641\n",
      "[200]\ttrain's multi_logloss: 0.292984\tvalid's multi_logloss: 0.32663\n",
      "[300]\ttrain's multi_logloss: 0.263743\tvalid's multi_logloss: 0.318631\n",
      "[400]\ttrain's multi_logloss: 0.241456\tvalid's multi_logloss: 0.314194\n",
      "[500]\ttrain's multi_logloss: 0.223769\tvalid's multi_logloss: 0.311146\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttrain's multi_logloss: 0.223769\tvalid's multi_logloss: 0.311146\n",
      "Training fold 4/5...\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttrain's multi_logloss: 0.3403\tvalid's multi_logloss: 0.346665\n",
      "[200]\ttrain's multi_logloss: 0.295454\tvalid's multi_logloss: 0.327945\n",
      "[300]\ttrain's multi_logloss: 0.264278\tvalid's multi_logloss: 0.319963\n",
      "[400]\ttrain's multi_logloss: 0.241407\tvalid's multi_logloss: 0.315422\n",
      "[500]\ttrain's multi_logloss: 0.223332\tvalid's multi_logloss: 0.312486\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttrain's multi_logloss: 0.223332\tvalid's multi_logloss: 0.312486\n",
      "Training fold 5/5...\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttrain's multi_logloss: 0.337895\tvalid's multi_logloss: 0.347803\n",
      "[200]\ttrain's multi_logloss: 0.292874\tvalid's multi_logloss: 0.329131\n",
      "[300]\ttrain's multi_logloss: 0.262865\tvalid's multi_logloss: 0.320962\n",
      "[400]\ttrain's multi_logloss: 0.240218\tvalid's multi_logloss: 0.316108\n",
      "[500]\ttrain's multi_logloss: 0.222132\tvalid's multi_logloss: 0.312719\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttrain's multi_logloss: 0.222132\tvalid's multi_logloss: 0.312719\n",
      "OOF Accuracy: 0.8561958983660891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17760\\1423733647.py:11: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  data_path = \"C:\\Machine Learning\\Phase 2\\Data\\phase2_students_before_cleaning.csv\"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'class_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 127\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 127\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 122\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    120\u001b[0m df \u001b[38;5;241m=\u001b[39m load_data(data_path)\n\u001b[0;32m    121\u001b[0m X, y, sample_weights, class_names \u001b[38;5;241m=\u001b[39m preprocess(df)\n\u001b[1;32m--> 122\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m model\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlgbm_cyber_attack_model.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 115\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(X, y, sample_weights, folds)\u001b[0m\n\u001b[0;32m    112\u001b[0m     oof_preds[val_idx] \u001b[38;5;241m=\u001b[39m preds\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOOF Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_score(y, oof_preds))\n\u001b[1;32m--> 115\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y, oof_preds, target_names\u001b[38;5;241m=\u001b[39m\u001b[43mclass_names\u001b[49m))\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[1;31mNameError\u001b[0m: name 'class_names' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import lightgbm as lgb\n",
    "\n",
    "# 1. Load data\n",
    "data_path = \"C:\\Machine Learning\\Phase 2\\Data\\phase2_students_before_cleaning.csv\"\n",
    "\n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "\n",
    "# 2. Data cleaning: dedupe, shuffle, winsorize + log1p\n",
    "num_cols = [\n",
    "    \"flow_time\", \"header_size\", \"packet_duration\", \"overall_rate\",\n",
    "    \"src_rate\", \"dst_rate\", \"fin_packets\", \"urg_packets\",\n",
    "    \"rst_packets\", \"max_value\", \"value_covariance\"\n",
    "]\n",
    "\n",
    "def clean_data(df):\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    for col in num_cols:\n",
    "        lower, upper = np.percentile(df[col], [1, 99])\n",
    "        df[col] = df[col].clip(lower, upper)\n",
    "        df[col] = np.log1p(df[col])\n",
    "    return df\n",
    "\n",
    "# 3. Preprocess: encode target, scale, class weights, feature selection\n",
    "def preprocess(df):\n",
    "    df = clean_data(df)\n",
    "    X = df.drop(columns=[\"label\"])\n",
    "    y = df[\"label\"].values\n",
    "\n",
    "    # Encode string labels to integers\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y)\n",
    "    class_names = le.classes_\n",
    "\n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "\n",
    "    # Handle class imbalance with sample weights\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.arange(len(class_names)),\n",
    "        y=y_enc\n",
    "    )\n",
    "    sample_weights = class_weights[y_enc]\n",
    "\n",
    "    # Feature selection using LightGBM importance\n",
    "    selector = lgb.LGBMClassifier(\n",
    "        objective=\"multiclass\",\n",
    "        num_class=len(class_names),\n",
    "        n_estimators=50,\n",
    "        learning_rate=0.1,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=42\n",
    "    )\n",
    "    selector.fit(X, y_enc)\n",
    "    sfm = SelectFromModel(selector, prefit=True, threshold=\"median\")\n",
    "    X_selected = sfm.transform(X)\n",
    "    selected_features = X.columns[sfm.get_support()]\n",
    "    X = pd.DataFrame(X_selected, columns=selected_features)\n",
    "\n",
    "    return X, y_enc, sample_weights, class_names\n",
    "\n",
    "# 4. Train with stratified K-fold and LightGBM\n",
    "def train_model(X, y, sample_weights, folds=5):\n",
    "    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "    oof_preds = np.zeros(len(y), dtype=int)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"Training fold {fold+1}/{folds}...\")\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        w_train = sample_weights[train_idx]\n",
    "\n",
    "        lgb_train = lgb.Dataset(X_train, label=y_train, weight=w_train)\n",
    "        lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n",
    "\n",
    "        params = {\n",
    "            \"objective\": \"multiclass\",\n",
    "            \"num_class\": len(np.unique(y)),\n",
    "            \"metric\": \"multi_logloss\",\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"num_leaves\": 31,\n",
    "            \"max_depth\": -1,\n",
    "            \"seed\": 42,\n",
    "            \"verbosity\": -1\n",
    "        }\n",
    "\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            lgb_train,\n",
    "            num_boost_round=500,\n",
    "            valid_sets=[lgb_train, lgb_val],\n",
    "            valid_names=[\"train\", \"valid\"],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=30),\n",
    "                lgb.log_evaluation(period=100)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        preds = np.argmax(model.predict(X_val), axis=1)\n",
    "        oof_preds[val_idx] = preds\n",
    "\n",
    "    print(\"OOF Accuracy:\", accuracy_score(y, oof_preds))\n",
    "    print(classification_report(y, oof_preds, target_names=class_names))\n",
    "    return model\n",
    "\n",
    "# 5. Main\n",
    "def main():\n",
    "    df = load_data(data_path)\n",
    "    X, y, sample_weights, class_names = preprocess(df)\n",
    "    model = train_model(X, y, sample_weights, folds=5)\n",
    "    model.save_model(\"lgbm_cyber_attack_model.txt\")\n",
    "    print(\"Model saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
